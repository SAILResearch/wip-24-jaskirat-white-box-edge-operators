nohup: ignoring input
+ main
+ init_params
+ dtype=int8
+ quant_format=QDQ
+ dataset_location=mrpc
+ input_model=../saved_results
+ approach=dynamic
+ output_model=roberta-base-mrpc-int8-qat.onnx
+ run_tuning
+ tuned_checkpoint=saved_results
+ extra_cmd=
+ batch_size=16
+ MAX_SEQ_LENGTH=128
+ model_name_or_path=../saved_results
+ TASK_NAME=mrpc
+ python -u ./run_glue.py --model_name_or_path ../saved_results --task_name mrpc --do_eval --do_train --max_seq_length 128 --per_device_eval_batch_size 16 --no_cuda --output_dir saved_results --output_model roberta-base-mrpc-int8-qat.onnx --export --export_dtype int8 --quant_format QDQ --output_dir saved_results --overwrite_output_dir --approach dynamic
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1270: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead
  warnings.warn(
11/10/2023 18:56:46 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False
[WARNING|modeling_utils.py:3765] 2023-11-10 18:56:49,300 >> Some weights of the model checkpoint at ../saved_results were not used when initializing RobertaForSequenceClassification: ['classifier.out_proj.zero_point', 'roberta.encoder.layer.7.output.LayerNorm.scale', 'roberta.encoder.layer.6.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.8.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.11.output.LayerNorm.scale', 'roberta.encoder.layer.8.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.8.attention.self.key.module.scale', 'roberta.encoder.layer.9.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.5.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.0.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.4.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.5.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.9.output.dense._packed_params.dtype', 'roberta.encoder.layer.5.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.9.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.0.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.5.intermediate.dense.scale', 'roberta.encoder.layer.4.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.9.attention.self.query.module.zero_point', 'roberta.encoder.layer.8.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.3.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.9.intermediate.dense.zero_point', 'classifier.out_proj.scale', 'roberta.encoder.layer.10.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.11.output.dense_input_scale_0', 'roberta.encoder.layer.1.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.8.output.LayerNorm.scale', 'roberta.encoder.layer.11.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.11.attention.self.value.module.scale', 'roberta.encoder.layer.7.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.0.output.LayerNorm.zero_point', 'roberta.encoder.layer.7.attention.self.query.module.scale', 'roberta.encoder.layer.7.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.8.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.11.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.11.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.9.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.4.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.6.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.7.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.9.attention.self.value.module.zero_point', 'roberta.encoder.layer.3.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.3.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.3.output.dense_input_zero_point_0', 'roberta.encoder.layer.9.attention.self.query.module.scale', 'roberta.encoder.layer.10.attention.output.dense_input_scale_0', 'roberta.encoder.layer.6.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.7.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.5.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.0.output.LayerNorm.scale', 'roberta.encoder.layer.11.output.dense._packed_params._packed_params', 'roberta.encoder.layer.1.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.5.attention.output.dense.scale', 'roberta.encoder.layer.0.output.dense.scale', 'roberta.encoder.layer.2.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.9.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.6.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.10.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.2.attention.self.value.module.zero_point', 'roberta.encoder.layer.8.output.LayerNorm.zero_point', 'roberta.encoder.layer.11.attention.output.LayerNorm.scale', 'roberta.embeddings.LayerNorm.module.weight', 'roberta.encoder.layer.8.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.6.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.10.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.2.output.dense._packed_params._packed_params', 'roberta.encoder.layer.9.attention.output.LayerNorm.scale', 'roberta.encoder.layer.8.attention.self.query.module.scale', 'roberta.encoder.layer.2.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.7.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.10.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.6.intermediate.dense.zero_point', 'roberta.encoder.layer.2.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.4.output.LayerNorm.scale', 'roberta.encoder.layer.8.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.3.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.1.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.11.output.dense.zero_point', 'roberta.encoder.layer.7.attention.self.key.module.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.zero_point', 'roberta.encoder.layer.7.attention.output.dense.zero_point', 'roberta.encoder.layer.10.intermediate.dense.scale', 'roberta.embeddings.position_embeddings.module._packed_params._packed_weight', 'roberta.encoder.layer.7.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.8.attention.output.dense_input_scale_0', 'roberta.encoder.layer.11.attention.output.dense.zero_point', 'roberta.encoder.layer.8.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.1.output.LayerNorm.scale', 'roberta.encoder.layer.9.intermediate.dense_input_scale_0', 'roberta.encoder.layer.9.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.5.attention.self.query.module.scale', 'roberta.encoder.layer.5.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.1.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.2.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.6.attention.output.dense.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.zero_point', 'roberta.encoder.layer.6.attention.output.dense_input_scale_0', 'roberta.encoder.layer.5.output.dense_input_scale_0', 'roberta.encoder.layer.2.output.dense.scale', 'roberta.encoder.layer.7.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.6.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.3.attention.self.query.module.scale', 'roberta.encoder.layer.0.attention.self.value.module.scale', 'roberta.encoder.layer.7.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.0.attention.output.dense.zero_point', 'roberta.encoder.layer.2.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.6.attention.self.value.module.scale', 'roberta.encoder.layer.8.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.10.attention.self.key.module.zero_point', 'roberta.encoder.layer.10.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.0.intermediate.dense.scale', 'roberta.encoder.layer.6.output.dense._packed_params.dtype', 'roberta.encoder.layer.10.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.4.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.5.attention.self.key.module.scale', 'roberta.encoder.layer.8.attention.output.dense.scale', 'roberta.embeddings.word_embeddings.module._packed_params.dtype', 'roberta.embeddings.LayerNorm.module_input_zero_point_0', 'roberta.encoder.layer.3.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.11.intermediate.dense.scale', 'roberta.encoder.layer.9.attention.self.value.module.scale', 'roberta.encoder.layer.1.intermediate.dense.zero_point', 'roberta.encoder.layer.4.attention.self.value.module._packed_params._packed_params', 'classifier.dense._packed_params._packed_params', 'roberta.encoder.layer.6.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.9.attention.self.key.module.zero_point', 'roberta.encoder.layer.8.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.3.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.7.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.10.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.3.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.10.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.5.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.5.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.1.attention.self.query.module.zero_point', 'roberta.encoder.layer.4.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.0.output.dense._packed_params._packed_params', 'roberta.encoder.layer.7.attention.self.value.module.zero_point', 'roberta.encoder.layer.7.attention.output.dense._packed_params.dtype', 'classifier.out_proj._packed_params._packed_params', 'roberta.encoder.layer.3.output.dense._packed_params.dtype', 'roberta.encoder.layer.2.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.8.output.dense_input_zero_point_0', 'roberta.encoder.layer.3.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.5.output.dense.scale', 'roberta.encoder.layer.1.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.6.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.8.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.1.attention.output.dense.zero_point', 'roberta.encoder.layer.6.output.dense._packed_params._packed_params', 'roberta.encoder.layer.1.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.9.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.0.attention.self.query.module.zero_point', 'roberta.encoder.layer.3.intermediate.dense.scale', 'roberta.encoder.layer.0.attention.self.query.module.scale', 'roberta.encoder.layer.11.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.2.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.8.attention.self.value.module.zero_point', 'roberta.encoder.layer.5.intermediate.dense_input_scale_0', 'roberta.encoder.layer.8.intermediate.dense_input_scale_0', 'roberta.encoder.layer.3.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.4.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.4.output.dense._packed_params.dtype', 'roberta.encoder.layer.1.attention.self.value.module.scale', 'roberta.encoder.layer.7.attention.self.value.module._packed_params._packed_params', 'roberta.embeddings.LayerNorm.module_input_scale_0', 'roberta.encoder.layer.4.attention.output.dense.scale', 'roberta.encoder.layer.8.output.dense.zero_point', 'roberta.encoder.layer.1.attention.self.key.module._packed_params.dtype', 'roberta.embeddings.LayerNorm.module.zero_point', 'roberta.encoder.layer.4.attention.self.value.module._packed_params.dtype', 'classifier.dense.scale', 'roberta.encoder.layer.1.output.dense.scale', 'roberta.encoder.layer.2.intermediate.dense_input_scale_0', 'roberta.encoder.layer.4.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.2.output.dense_input_zero_point_0', 'roberta.encoder.layer.0.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.2.attention.output.dense_input_scale_0', 'roberta.encoder.layer.9.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.8.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.11.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.0.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.3.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.5.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.7.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.11.attention.self.key.module.zero_point', 'roberta.encoder.layer.0.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.1.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.2.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.10.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.7.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.0.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.5.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.11.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.2.intermediate.dense.zero_point', 'roberta.encoder.layer.11.output.dense.scale', 'roberta.encoder.layer.0.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.9.output.dense_input_scale_0', 'roberta.encoder.layer.6.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.8.intermediate.dense.scale', 'roberta.encoder.layer.3.intermediate.dense_input_scale_0', 'roberta.encoder.layer.5.attention.self.query.module.zero_point', 'classifier.dense._packed_params.dtype', 'roberta.encoder.layer.5.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.0.attention.output.LayerNorm.scale', 'roberta.encoder.layer.0.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.6.intermediate.dense.scale', 'roberta.encoder.layer.6.output.dense_input_scale_0', 'roberta.encoder.layer.6.intermediate.dense_input_scale_0', 'roberta.encoder.layer.3.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.7.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.10.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.10.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.10.output.dense.zero_point', 'roberta.encoder.layer.7.output.dense._packed_params.dtype', 'roberta.encoder.layer.1.intermediate.dense_input_scale_0', 'roberta.encoder.layer.0.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.4.attention.output.LayerNorm.scale', 'roberta.encoder.layer.6.output.dense.scale', 'roberta.encoder.layer.11.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.9.output.LayerNorm.scale', 'roberta.encoder.layer.1.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.7.attention.self.query.module.zero_point', 'roberta.encoder.layer.1.intermediate.dense.scale', 'roberta.encoder.layer.7.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.11.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.5.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.6.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.0.attention.self.key.module.scale', 'roberta.encoder.layer.5.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.1.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.9.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.2.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.8.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.10.attention.self.value.module.zero_point', 'roberta.encoder.layer.1.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.2.attention.self.query.module.scale', 'roberta.encoder.layer.2.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.10.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.3.output.dense.zero_point', 'classifier.dense.zero_point', 'roberta.encoder.layer.11.attention.output.dense_input_scale_0', 'roberta.encoder.layer.11.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.6.attention.output.LayerNorm.scale', 'roberta.encoder.layer.5.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.7.output.dense_input_zero_point_0', 'roberta.encoder.layer.1.output.dense_input_scale_0', 'roberta.encoder.layer.2.output.LayerNorm.zero_point', 'roberta.encoder.layer.3.attention.output.LayerNorm.scale', 'roberta.encoder.layer.5.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.10.output.LayerNorm.scale', 'roberta.encoder.layer.10.attention.self.value.module.scale', 'roberta.encoder.layer.4.intermediate.dense.scale', 'roberta.encoder.layer.1.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.4.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.8.output.dense._packed_params.dtype', 'roberta.encoder.layer.7.output.LayerNorm.zero_point', 'roberta.encoder.layer.5.attention.self.value.module.scale', 'roberta.encoder.layer.6.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.6.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.10.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.7.intermediate.dense.zero_point', 'roberta.encoder.layer.4.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.6.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.1.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.4.output.dense.scale', 'roberta.encoder.layer.5.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.8.attention.self.key.module.zero_point', 'roberta.encoder.layer.3.output.LayerNorm.zero_point', 'roberta.encoder.layer.8.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.9.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.0.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.1.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.4.attention.self.key.module.zero_point', 'roberta.encoder.layer.5.output.dense._packed_params._packed_params', 'roberta.encoder.layer.11.attention.self.query.module.scale', 'roberta.encoder.layer.8.output.dense.scale', 'roberta.encoder.layer.9.attention.output.dense_input_scale_0', 'roberta.encoder.layer.0.intermediate.dense.zero_point', 'roberta.encoder.layer.3.attention.self.query.module.zero_point', 'roberta.encoder.layer.0.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.0.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.5.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.11.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.9.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.9.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.11.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.2.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.8.output.dense._packed_params._packed_params', 'roberta.encoder.layer.10.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.2.attention.self.key.module.scale', 'roberta.encoder.layer.2.intermediate.dense.scale', 'roberta.encoder.layer.9.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.7.attention.output.dense.scale', 'roberta.encoder.layer.2.output.dense._packed_params.dtype', 'roberta.encoder.layer.3.attention.output.dense.scale', 'roberta.encoder.layer.1.attention.self.query.module.scale', 'roberta.encoder.layer.3.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.0.output.dense_input_zero_point_0', 'roberta.encoder.layer.7.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.10.output.LayerNorm.zero_point', 'roberta.encoder.layer.11.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.8.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.10.output.dense_input_scale_0', 'roberta.encoder.layer.8.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.0.attention.output.dense_input_zero_point_0', 'roberta.embeddings.token_type_embeddings.module._packed_params._packed_weight', 'roberta.encoder.layer.0.output.dense._packed_params.dtype', 'roberta.encoder.layer.4.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.4.attention.output.dense._packed_params._packed_params', 'classifier.out_proj._packed_params.dtype', 'roberta.encoder.layer.9.intermediate.dense.scale', 'roberta.encoder.layer.4.attention.output.dense_input_scale_0', 'roberta.encoder.layer.6.output.dense.zero_point', 'roberta.encoder.layer.5.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.6.attention.self.query.module.scale', 'roberta.encoder.layer.1.attention.output.dense_input_scale_0', 'roberta.encoder.layer.3.attention.output.dense_input_scale_0', 'roberta.encoder.layer.10.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.10.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.1.attention.output.dense.scale', 'roberta.encoder.layer.6.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.11.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.1.attention.output.LayerNorm.scale', 'roberta.encoder.layer.3.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.3.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.4.output.dense._packed_params._packed_params', 'roberta.encoder.layer.3.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.1.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.3.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.6.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.8.output.dense_input_scale_0', 'roberta.encoder.layer.3.attention.self.key.module.scale', 'roberta.encoder.layer.2.output.dense_input_scale_0', 'roberta.encoder.layer.8.intermediate.dense.zero_point', 'roberta.encoder.layer.5.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.10.intermediate.dense.zero_point', 'roberta.encoder.layer.2.attention.output.LayerNorm.scale', 'roberta.encoder.layer.8.attention.self.key.module_input_scale_0', 'roberta.embeddings.LayerNorm.module.scale', 'roberta.encoder.layer.7.intermediate.dense_input_scale_0', 'roberta.encoder.layer.10.output.dense._packed_params.dtype', 'roberta.encoder.layer.3.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.4.output.dense_input_zero_point_0', 'roberta.encoder.layer.7.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.5.intermediate.dense.zero_point', 'roberta.encoder.layer.7.output.dense_input_scale_0', 'roberta.encoder.layer.9.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.1.output.dense_input_zero_point_0', 'roberta.encoder.layer.5.attention.self.key.module.zero_point', 'roberta.encoder.layer.10.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.10.intermediate.dense_input_scale_0', 'roberta.encoder.layer.0.attention.self.query.module._packed_params._packed_params', 'roberta.embeddings.LayerNorm.module.bias', 'classifier.dropout_input_zero_point_0', 'roberta.encoder.layer.3.attention.self.value.module.scale', 'roberta.encoder.layer.11.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.0.output.dense_input_scale_0', 'roberta.encoder.layer.11.output.LayerNorm.zero_point', 'roberta.encoder.layer.2.attention.self.value.module.scale', 'roberta.encoder.layer.9.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.1.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.4.output.dense.zero_point', 'roberta.encoder.layer.11.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.11.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.7.output.dense.scale', 'roberta.encoder.layer.6.attention.output.dense.scale', 'roberta.encoder.layer.1.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.9.attention.output.dense.scale', 'roberta.encoder.layer.9.output.dense_input_zero_point_0', 'roberta.encoder.layer.1.output.LayerNorm.zero_point', 'roberta.encoder.layer.6.output.dense_input_zero_point_0', 'roberta.encoder.layer.0.attention.self.value.module.zero_point', 'roberta.encoder.layer.2.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.7.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.1.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.1.attention.self.key.module.zero_point', 'roberta.encoder.layer.2.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.5.output.LayerNorm.zero_point', 'roberta.encoder.layer.0.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.5.output.LayerNorm.scale', 'roberta.encoder.layer.4.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.11.intermediate.dense.zero_point', 'roberta.embeddings.token_type_embeddings.module._packed_params.dtype', 'roberta.encoder.layer.9.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.1.output.dense._packed_params._packed_params', 'roberta.encoder.layer.10.attention.self.key.module.scale', 'roberta.encoder.layer.2.output.dense.zero_point', 'roberta.encoder.layer.5.attention.self.value.module.zero_point', 'roberta.encoder.layer.8.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.11.attention.self.value.module.zero_point', 'roberta.encoder.layer.6.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.10.attention.output.LayerNorm.scale', 'roberta.encoder.layer.11.attention.output.dense.scale', 'roberta.encoder.layer.2.attention.self.value.module._packed_params.dtype', 'classifier.dropout_input_scale_0', 'roberta.encoder.layer.0.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.3.attention.output.dense.zero_point', 'roberta.encoder.layer.0.intermediate.dense_input_scale_0', 'roberta.encoder.layer.2.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.4.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.2.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.8.attention.self.value.module.scale', 'roberta.encoder.layer.2.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.6.output.LayerNorm.scale', 'roberta.encoder.layer.9.output.dense.zero_point', 'roberta.encoder.layer.11.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.9.attention.output.dense.zero_point', 'roberta.encoder.layer.7.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.9.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.7.attention.self.key.module.scale', 'roberta.encoder.layer.6.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.4.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.4.attention.self.query.module.scale', 'roberta.encoder.layer.6.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.4.attention.self.query.module.zero_point', 'roberta.encoder.layer.0.attention.output.dense_input_scale_0', 'roberta.encoder.layer.1.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.0.attention.self.key.module.zero_point', 'roberta.encoder.layer.2.attention.self.key.module.zero_point', 'roberta.encoder.layer.8.attention.self.query.module.zero_point', 'roberta.encoder.layer.9.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.10.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.11.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.2.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.2.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.1.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.3.attention.self.key.module.zero_point', 'roberta.encoder.layer.10.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.9.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.8.attention.self.key.module._packed_params._packed_params', 'roberta.encoder.layer.1.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.10.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.5.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.1.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.7.attention.output.dense_input_scale_0', 'roberta.encoder.layer.7.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.7.output.dense.zero_point', 'roberta.encoder.layer.4.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.7.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.10.attention.self.query.module.scale', 'roberta.encoder.layer.2.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.5.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.1.output.dense._packed_params.dtype', 'roberta.encoder.layer.2.attention.self.query.module.zero_point', 'roberta.embeddings.position_embeddings.module._packed_params.dtype', 'roberta.encoder.layer.7.intermediate.dense.scale', 'roberta.encoder.layer.2.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.9.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.5.attention.output.LayerNorm.scale', 'roberta.encoder.layer.2.attention.output.dense.scale', 'roberta.encoder.layer.3.attention.self.key.module._packed_params.dtype', 'roberta.encoder.layer.7.attention.self.value.module.scale', 'roberta.encoder.layer.6.attention.self.key.module.zero_point', 'roberta.encoder.layer.4.intermediate.dense_input_scale_0', 'roberta.encoder.layer.4.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.10.attention.self.query.module.zero_point', 'best_configure', 'roberta.encoder.layer.0.attention.self.key.module_input_scale_0', 'roberta.encoder.layer.0.output.dense.zero_point', 'roberta.encoder.layer.11.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.6.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.11.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.9.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.8.attention.output.dense.zero_point', 'roberta.encoder.layer.2.output.LayerNorm.scale', 'roberta.encoder.layer.8.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.11.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.2.attention.output.dense.zero_point', 'roberta.encoder.layer.11.output.dense_input_zero_point_0', 'roberta.encoder.layer.4.intermediate.dense_input_zero_point_0', 'roberta.encoder.layer.10.attention.output.dense.zero_point', 'roberta.encoder.layer.3.output.LayerNorm.scale', 'roberta.encoder.layer.3.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.3.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.0.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.1.output.dense.zero_point', 'roberta.encoder.layer.4.output.LayerNorm.zero_point', 'roberta.encoder.layer.5.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.5.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.9.output.dense._packed_params._packed_params', 'roberta.encoder.layer.6.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.8.attention.output.LayerNorm.scale', 'roberta.encoder.layer.9.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.11.attention.self.query.module.zero_point', 'roberta.encoder.layer.3.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.5.output.dense._packed_params.dtype', 'roberta.encoder.layer.6.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.4.attention.self.key.module_input_zero_point_0', 'roberta.encoder.layer.6.output.LayerNorm.zero_point', 'roberta.encoder.layer.10.output.dense._packed_params._packed_params', 'roberta.encoder.layer.3.output.dense._packed_params._packed_params', 'roberta.encoder.layer.7.output.dense._packed_params._packed_params', 'roberta.encoder.layer.5.output.dense.zero_point', 'roberta.encoder.layer.3.intermediate.dense._packed_params._packed_params', 'roberta.encoder.layer.4.attention.self.value.module.scale', 'roberta.encoder.layer.8.attention.self.value.module._packed_params._packed_params', 'roberta.encoder.layer.3.attention.output.LayerNorm.zero_point', 'roberta.encoder.layer.0.attention.self.value.module._packed_params.dtype', 'roberta.encoder.layer.4.intermediate.dense.zero_point', 'roberta.encoder.layer.4.attention.output.dense.zero_point', 'roberta.encoder.layer.2.attention.output.dense._packed_params.dtype', 'roberta.encoder.layer.11.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.3.intermediate.dense.zero_point', 'roberta.encoder.layer.6.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.5.attention.output.dense.zero_point', 'roberta.encoder.layer.10.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.9.output.dense.scale', 'roberta.encoder.layer.7.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.9.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.5.output.dense_input_zero_point_0', 'roberta.encoder.layer.2.attention.self.query.module._packed_params._packed_params', 'roberta.encoder.layer.5.attention.output.dense_input_scale_0', 'roberta.encoder.layer.1.attention.self.value.module.zero_point', 'roberta.encoder.layer.3.attention.self.value.module.zero_point', 'roberta.encoder.layer.0.attention.output.dense.scale', 'roberta.encoder.layer.11.intermediate.dense_input_scale_0', 'roberta.encoder.layer.10.attention.output.dense.scale', 'roberta.encoder.layer.6.attention.self.key.module.scale', 'roberta.encoder.layer.6.attention.self.value.module_input_zero_point_0', 'roberta.embeddings.word_embeddings.module._packed_params._packed_weight', 'roberta.encoder.layer.9.output.LayerNorm.zero_point', 'roberta.encoder.layer.4.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.10.attention.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.0.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.4.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.11.attention.self.query.module._packed_params.dtype', 'roberta.encoder.layer.3.output.dense.scale', 'roberta.encoder.layer.11.attention.self.key.module.scale', 'roberta.encoder.layer.6.attention.self.query.module.zero_point', 'roberta.encoder.layer.11.output.dense._packed_params.dtype', 'roberta.encoder.layer.3.output.dense_input_scale_0', 'roberta.encoder.layer.4.attention.self.key.module.scale', 'roberta.encoder.layer.1.attention.self.key.module.scale', 'roberta.encoder.layer.0.intermediate.dense._packed_params.dtype', 'roberta.encoder.layer.5.attention.self.query.module_input_scale_0', 'roberta.encoder.layer.8.attention.self.value.module_input_zero_point_0', 'roberta.encoder.layer.0.output.LayerNorm_input_scale_0', 'roberta.encoder.layer.0.attention.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.9.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.4.output.dense_input_scale_0', 'roberta.encoder.layer.7.attention.output.LayerNorm.scale', 'roberta.encoder.layer.1.attention.self.value.module_input_scale_0', 'roberta.encoder.layer.8.attention.output.dense._packed_params._packed_params', 'roberta.encoder.layer.10.output.LayerNorm_input_zero_point_0', 'roberta.encoder.layer.9.attention.self.key.module.scale', 'roberta.encoder.layer.7.attention.self.query.module_input_zero_point_0', 'roberta.encoder.layer.10.output.dense.scale', 'roberta.encoder.layer.4.attention.output.dense_input_zero_point_0', 'roberta.encoder.layer.10.output.dense_input_zero_point_0', 'roberta.encoder.layer.8.intermediate.dense._packed_params._packed_params']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3777] 2023-11-10 18:56:49,301 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../saved_results and are newly initialized: ['roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'classifier.out_proj.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'classifier.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'classifier.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/10/2023 18:56:49 - WARNING - __main__ - Your model seems to have been trained with labels, but they don't match the dataset: model labels: ['0', '1'], dataset labels: ['equivalent', 'not_equivalent'].
Ignoring the model labels as a result.
/home/21js160/qat_roberta/export_model/./run_glue.py:458: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("glue", data_args.task_name)
2023-11-10 18:56:51 [INFO] Start auto tuning.
2023-11-10 18:56:51 [INFO] Execute the tuning process due to detect the evaluation function.
2023-11-10 18:56:51 [INFO] Adaptor has 5 recipes.
2023-11-10 18:56:51 [INFO] 0 recipes specified by user.
2023-11-10 18:56:51 [INFO] 3 recipes require future tuning.
2023-11-10 18:56:51 [INFO] *** Initialize auto tuning
2023-11-10 18:56:51 [INFO] {
2023-11-10 18:56:51 [INFO]     'PostTrainingQuantConfig': {
2023-11-10 18:56:51 [INFO]         'AccuracyCriterion': {
2023-11-10 18:56:51 [INFO]             'criterion': 'relative',
2023-11-10 18:56:51 [INFO]             'higher_is_better': True,
2023-11-10 18:56:51 [INFO]             'tolerable_loss': 0.01,
2023-11-10 18:56:51 [INFO]             'absolute': None,
2023-11-10 18:56:51 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7fb19d7310c0>>,
2023-11-10 18:56:51 [INFO]             'relative': 0.01
2023-11-10 18:56:51 [INFO]         },
2023-11-10 18:56:51 [INFO]         'approach': 'post_training_dynamic_quant',
2023-11-10 18:56:51 [INFO]         'backend': 'default',
2023-11-10 18:56:51 [INFO]         'calibration_sampling_size': [
2023-11-10 18:56:51 [INFO]             100
2023-11-10 18:56:51 [INFO]         ],
2023-11-10 18:56:51 [INFO]         'device': 'cpu',
2023-11-10 18:56:51 [INFO]         'diagnosis': False,
2023-11-10 18:56:51 [INFO]         'domain': 'auto',
2023-11-10 18:56:51 [INFO]         'example_inputs': None,
2023-11-10 18:56:51 [INFO]         'excluded_precisions': [
2023-11-10 18:56:51 [INFO]         ],
2023-11-10 18:56:51 [INFO]         'framework': 'pytorch_fx',
2023-11-10 18:56:51 [INFO]         'inputs': [
2023-11-10 18:56:51 [INFO]         ],
2023-11-10 18:56:51 [INFO]         'model_name': '',
2023-11-10 18:56:51 [INFO]         'ni_workload_name': 'quantization',
2023-11-10 18:56:51 [INFO]         'op_name_dict': None,
2023-11-10 18:56:51 [INFO]         'op_type_dict': None,
2023-11-10 18:56:51 [INFO]         'outputs': [
2023-11-10 18:56:51 [INFO]         ],
2023-11-10 18:56:51 [INFO]         'quant_format': 'default',
2023-11-10 18:56:51 [INFO]         'quant_level': 'auto',
2023-11-10 18:56:51 [INFO]         'recipes': {
2023-11-10 18:56:51 [INFO]             'smooth_quant': False,
2023-11-10 18:56:51 [INFO]             'smooth_quant_args': {
2023-11-10 18:56:51 [INFO]             },
2023-11-10 18:56:51 [INFO]             'layer_wise_quant': False,
2023-11-10 18:56:51 [INFO]             'layer_wise_quant_args': {
2023-11-10 18:56:51 [INFO]             },
2023-11-10 18:56:51 [INFO]             'fast_bias_correction': False,
2023-11-10 18:56:51 [INFO]             'weight_correction': False,
2023-11-10 18:56:51 [INFO]             'gemm_to_matmul': True,
2023-11-10 18:56:51 [INFO]             'graph_optimization_level': None,
2023-11-10 18:56:51 [INFO]             'first_conv_or_matmul_quantization': True,
2023-11-10 18:56:51 [INFO]             'last_conv_or_matmul_quantization': True,
2023-11-10 18:56:51 [INFO]             'pre_post_process_quantization': True,
2023-11-10 18:56:51 [INFO]             'add_qdq_pair_to_weight': False,
2023-11-10 18:56:51 [INFO]             'optypes_to_exclude_output_quant': [
2023-11-10 18:56:51 [INFO]             ],
2023-11-10 18:56:51 [INFO]             'dedicated_qdq_pair': False,
2023-11-10 18:56:51 [INFO]             'rtn_args': {
2023-11-10 18:56:51 [INFO]             },
2023-11-10 18:56:51 [INFO]             'awq_args': {
2023-11-10 18:56:51 [INFO]             },
2023-11-10 18:56:51 [INFO]             'gptq_args': {
2023-11-10 18:56:51 [INFO]             },
2023-11-10 18:56:51 [INFO]             'teq_args': {
2023-11-10 18:56:51 [INFO]             }
2023-11-10 18:56:51 [INFO]         },
2023-11-10 18:56:51 [INFO]         'reduce_range': None,
2023-11-10 18:56:51 [INFO]         'TuningCriterion': {
2023-11-10 18:56:51 [INFO]             'max_trials': 100,
2023-11-10 18:56:51 [INFO]             'objective': [
2023-11-10 18:56:51 [INFO]                 'performance'
2023-11-10 18:56:51 [INFO]             ],
2023-11-10 18:56:51 [INFO]             'strategy': 'basic',
2023-11-10 18:56:51 [INFO]             'strategy_kwargs': None,
2023-11-10 18:56:51 [INFO]             'timeout': 0
2023-11-10 18:56:51 [INFO]         },
2023-11-10 18:56:51 [INFO]         'use_bf16': True
2023-11-10 18:56:51 [INFO]     }
2023-11-10 18:56:51 [INFO] }
2023-11-10 18:56:51 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.
2023-11-10 18:56:51 [INFO]  Found 12 blocks
2023-11-10 18:56:51 [INFO] Attention Blocks: 12
2023-11-10 18:56:51 [INFO] FFN Blocks: 12
2023-11-10 18:56:51 [INFO] Pass query framework capability elapsed time: 364.44 ms
2023-11-10 18:56:51 [INFO] Get FP32 model baseline.
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:02,  8.78it/s] 12%|█▏        | 3/26 [00:00<00:03,  6.21it/s] 15%|█▌        | 4/26 [00:00<00:04,  5.30it/s] 19%|█▉        | 5/26 [00:00<00:04,  4.86it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.99it/s] 27%|██▋       | 7/26 [00:01<00:03,  4.91it/s] 31%|███       | 8/26 [00:01<00:03,  4.78it/s] 35%|███▍      | 9/26 [00:01<00:03,  4.82it/s] 38%|███▊      | 10/26 [00:02<00:03,  4.52it/s] 42%|████▏     | 11/26 [00:02<00:03,  4.48it/s] 46%|████▌     | 12/26 [00:02<00:03,  4.14it/s] 50%|█████     | 13/26 [00:02<00:03,  4.21it/s] 54%|█████▍    | 14/26 [00:02<00:02,  4.33it/s] 58%|█████▊    | 15/26 [00:03<00:02,  4.32it/s] 62%|██████▏   | 16/26 [00:03<00:02,  4.47it/s] 65%|██████▌   | 17/26 [00:03<00:02,  4.47it/s] 69%|██████▉   | 18/26 [00:03<00:01,  4.72it/s] 73%|███████▎  | 19/26 [00:04<00:01,  4.61it/s] 77%|███████▋  | 20/26 [00:04<00:01,  4.72it/s] 81%|████████  | 21/26 [00:04<00:01,  4.24it/s] 85%|████████▍ | 22/26 [00:04<00:00,  4.33it/s] 88%|████████▊ | 23/26 [00:04<00:00,  4.43it/s] 92%|█████████▏| 24/26 [00:05<00:00,  4.62it/s] 96%|█████████▌| 25/26 [00:05<00:00,  4.40it/s]100%|██████████| 26/26 [00:05<00:00,  4.62it/s]wandb: Currently logged in as: jkrt-ngh69 (abc_12). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/21js160/qat_roberta/export_model/wandb/run-20231110_185658-8h3mqoc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-cloud-362
wandb: ⭐️ View project at https://wandb.ai/abc_12/huggingface
wandb: 🚀 View run at https://wandb.ai/abc_12/huggingface/runs/8h3mqoc8
100%|██████████| 26/26 [00:14<00:00,  1.79it/s]
Batch size = 16
Finally Eval eval_f1 Accuracy: 0.8122270742358079
Latency: 14.509 ms
Throughput: 68.925 samples/sec
2023-11-10 18:57:06 [INFO] Save tuning history to /home/21js160/qat_roberta/export_model/nc_workspace/2023-11-10_18-56-49/./history.snapshot.
2023-11-10 18:57:06 [INFO] FP32 baseline is: [Accuracy: 0.8122, Duration (seconds): 14.8280]
2023-11-10 18:57:06 [INFO] Quantize the model with default config.
2023-11-10 18:57:06 [INFO] Fx trace of the entire model failed, We will conduct auto quantization
2023-11-10 18:57:09 [INFO] |******Mixed Precision Statistics******|
2023-11-10 18:57:09 [INFO] +-----------------+----------+---------+
2023-11-10 18:57:09 [INFO] |     Op Type     |  Total   |   INT8  |
2023-11-10 18:57:09 [INFO] +-----------------+----------+---------+
2023-11-10 18:57:09 [INFO] |    Embedding    |    3     |    3    |
2023-11-10 18:57:09 [INFO] |      Linear     |    74    |    74   |
2023-11-10 18:57:09 [INFO] +-----------------+----------+---------+
2023-11-10 18:57:09 [INFO] Pass quantize model elapsed time: 2990.44 ms
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:02,  9.88it/s] 12%|█▏        | 3/26 [00:00<00:03,  6.67it/s] 15%|█▌        | 4/26 [00:00<00:03,  5.81it/s] 19%|█▉        | 5/26 [00:00<00:03,  5.33it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.88it/s] 27%|██▋       | 7/26 [00:01<00:03,  4.84it/s] 31%|███       | 8/26 [00:01<00:03,  4.85it/s] 35%|███▍      | 9/26 [00:01<00:03,  4.92it/s] 38%|███▊      | 10/26 [00:01<00:03,  4.85it/s] 42%|████▏     | 11/26 [00:02<00:03,  4.88it/s] 46%|████▌     | 12/26 [00:02<00:02,  4.92it/s] 50%|█████     | 13/26 [00:02<00:02,  4.77it/s] 54%|█████▍    | 14/26 [00:02<00:02,  4.72it/s] 58%|█████▊    | 15/26 [00:02<00:02,  4.60it/s] 62%|██████▏   | 16/26 [00:03<00:02,  4.73it/s] 65%|██████▌   | 17/26 [00:03<00:01,  4.58it/s] 69%|██████▉   | 18/26 [00:03<00:01,  4.57it/s] 73%|███████▎  | 19/26 [00:03<00:01,  4.58it/s] 77%|███████▋  | 20/26 [00:04<00:01,  4.55it/s] 81%|████████  | 21/26 [00:04<00:01,  4.54it/s] 85%|████████▍ | 22/26 [00:04<00:00,  4.47it/s] 88%|████████▊ | 23/26 [00:04<00:00,  4.47it/s] 92%|█████████▏| 24/26 [00:04<00:00,  4.49it/s] 96%|█████████▌| 25/26 [00:05<00:00,  4.47it/s]100%|██████████| 26/26 [00:05<00:00,  5.26it/s]100%|██████████| 26/26 [00:05<00:00,  4.88it/s]
Batch size = 16
Finally Eval eval_f1 Accuracy: 0.8122270742358079
Latency: 13.644 ms
Throughput: 73.291 samples/sec
2023-11-10 18:57:15 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.8122|0.8122, Duration (seconds) (int8|fp32): 5.5724|14.8280], Best tune result is: [Accuracy: 0.8122, Duration (seconds): 5.5724]
2023-11-10 18:57:15 [INFO] |**********************Tune Result Statistics**********************|
2023-11-10 18:57:15 [INFO] +--------------------+----------+---------------+------------------+
2023-11-10 18:57:15 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2023-11-10 18:57:15 [INFO] +--------------------+----------+---------------+------------------+
2023-11-10 18:57:15 [INFO] |      Accuracy      | 0.8122   |    0.8122     |     0.8122       |
2023-11-10 18:57:15 [INFO] | Duration (seconds) | 14.8280  |    5.5724     |     5.5724       |
2023-11-10 18:57:15 [INFO] +--------------------+----------+---------------+------------------+
2023-11-10 18:57:15 [INFO] [Strategy] Found a model that meets the accuracy requirements.
2023-11-10 18:57:15 [INFO] Save tuning history to /home/21js160/qat_roberta/export_model/nc_workspace/2023-11-10_18-56-49/./history.snapshot.
2023-11-10 18:57:15 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.
2023-11-10 18:57:15 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2023-11-10 18:57:15 [INFO] Save deploy yaml to /home/21js160/qat_roberta/export_model/nc_workspace/2023-11-10_18-56-49/deploy.yaml
2023-11-10 18:57:22 [INFO] Quantization format is not available when executing dynamic quantization.
11/10/2023 18:57:25 - WARNING - root - Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md 
2023-11-10 18:57:48 [INFO] *********************************************************************
2023-11-10 18:57:48 [INFO] The INT8 ONNX Model exported to path: roberta-base-mrpc-int8-qat.onnx
2023-11-10 18:57:48 [INFO] *********************************************************************
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:           eval/accuracy ▁▁
wandb:     eval/combined_score ▁▁
wandb:                 eval/f1 ▁▁
wandb:               eval/loss ▁█
wandb:            eval/runtime █▁
wandb: eval/samples_per_second ▁█
wandb:   eval/steps_per_second ▁█
wandb:       train/global_step ▁▁
wandb: 
wandb: Run summary:
wandb:           eval/accuracy 0.68382
wandb:     eval/combined_score 0.74803
wandb:                 eval/f1 0.81223
wandb:               eval/loss 0.67953
wandb:            eval/runtime 5.5668
wandb: eval/samples_per_second 73.291
wandb:   eval/steps_per_second 4.671
wandb:       train/global_step 0
wandb: 
wandb: 🚀 View run pious-cloud-362 at: https://wandb.ai/abc_12/huggingface/runs/8h3mqoc8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231110_185658-8h3mqoc8/logs
