nohup: ignoring input
+ main
+ init_params
+ dtype=int8
+ quant_format=QDQ
+ dataset_location=mrpc
+ input_model=../saved_results
+ approach=dynamic
+ output_model=bert-base-uncased-mrpc-int8-qat.onnx
+ run_tuning
+ tuned_checkpoint=saved_results
+ extra_cmd=
+ batch_size=16
+ MAX_SEQ_LENGTH=128
+ model_name_or_path=../saved_results
+ TASK_NAME=mrpc
+ python -u ./run_glue.py --model_name_or_path ../saved_results --task_name mrpc --do_eval --do_train --max_seq_length 128 --per_device_eval_batch_size 16 --no_cuda --output_dir saved_results --output_model bert-base-uncased-mrpc-int8-qat.onnx --export --export_dtype int8 --quant_format QDQ --output_dir saved_results --overwrite_output_dir --approach dynamic
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1270: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead
  warnings.warn(
11/10/2023 18:54:23 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False
[WARNING|modeling_utils.py:3765] 2023-11-10 18:54:26,004 >> Some weights of the model checkpoint at ../saved_results were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.2.attention.self.query.module.scale', 'bert.encoder.layer.2.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.8.intermediate.dense_input_scale_0', 'bert.encoder.layer.10.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.11.output.LayerNorm.zero_point', 'bert.encoder.layer.5.attention.self.value.module.scale', 'bert.encoder.layer.8.output.dense_input_scale_0', 'bert.encoder.layer.9.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.4.attention.output.dense_input_scale_0', 'bert.encoder.layer.0.output.dense_input_zero_point_0', 'bert.encoder.layer.3.output.LayerNorm.zero_point', 'bert.encoder.layer.7.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.0.attention.self.query.module_input_scale_0', 'bert.encoder.layer.7.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.9.attention.output.dense.scale', 'bert.encoder.layer.5.output.dense._packed_params._packed_params', 'bert.encoder.layer.4.output.dense._packed_params.dtype', 'bert.encoder.layer.7.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.8.output.LayerNorm.scale', 'bert.encoder.layer.6.attention.self.value.module_input_scale_0', 'bert.encoder.layer.3.attention.self.value.module.zero_point', 'bert.encoder.layer.2.attention.self.value.module_input_scale_0', 'bert.encoder.layer.1.attention.self.value.module_input_scale_0', 'bert.encoder.layer.8.attention.self.value.module.scale', 'bert.encoder.layer.4.attention.self.key.module.scale', 'bert.encoder.layer.4.attention.self.key.module.zero_point', 'bert.encoder.layer.2.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.9.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.3.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.2.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.9.intermediate.dense.scale', 'bert.encoder.layer.0.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.10.output.dense.zero_point', 'bert.encoder.layer.5.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.7.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.4.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.7.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.0.attention.output.dense.zero_point', 'bert.encoder.layer.11.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.11.output.LayerNorm_input_scale_0', 'bert.embeddings.position_embeddings.module._packed_params.dtype', 'bert.encoder.layer.7.output.dense._packed_params.dtype', 'bert.encoder.layer.1.attention.self.key.module_input_scale_0', 'bert.encoder.layer.5.output.LayerNorm_input_scale_0', 'bert.encoder.layer.4.output.LayerNorm.zero_point', 'bert.encoder.layer.4.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.5.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.5.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.8.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.10.output.dense.scale', 'bert.encoder.layer.2.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.10.attention.self.query.module.scale', 'bert.encoder.layer.2.intermediate.dense_input_scale_0', 'bert.encoder.layer.9.attention.self.query.module.scale', 'bert.encoder.layer.11.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.9.attention.self.value.module.zero_point', 'bert.encoder.layer.10.output.dense_input_scale_0', 'bert.encoder.layer.2.attention.output.dense_input_scale_0', 'bert.encoder.layer.9.intermediate.dense_input_scale_0', 'bert.encoder.layer.0.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.7.attention.output.dense.zero_point', 'bert.encoder.layer.0.attention.self.value.module.zero_point', 'bert.encoder.layer.3.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.8.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.1.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.1.output.dense.scale', 'bert.encoder.layer.4.attention.self.value.module_input_scale_0', 'bert.encoder.layer.8.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.3.output.dense_input_scale_0', 'bert.encoder.layer.2.attention.self.query.module.zero_point', 'bert.encoder.layer.2.attention.self.value.module.zero_point', 'bert.embeddings.token_type_embeddings.module._packed_params.dtype', 'bert.encoder.layer.0.attention.output.LayerNorm_input_scale_0', 'bert.pooler.dense._packed_params.dtype', 'bert.encoder.layer.5.attention.output.dense_input_scale_0', 'bert.encoder.layer.7.attention.output.LayerNorm.scale', 'bert.encoder.layer.4.attention.self.query.module.scale', 'bert.encoder.layer.8.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.11.attention.output.dense.zero_point', 'bert.encoder.layer.5.attention.self.key.module.zero_point', 'bert.encoder.layer.5.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.7.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.0.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.2.output.LayerNorm.scale', 'bert.encoder.layer.3.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.8.attention.self.value.module.zero_point', 'bert.encoder.layer.2.attention.self.key.module.zero_point', 'bert.encoder.layer.8.attention.output.dense.zero_point', 'bert.encoder.layer.8.attention.output.dense.scale', 'bert.encoder.layer.6.attention.self.key.module.scale', 'bert.encoder.layer.4.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.6.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.2.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.11.attention.self.key.module_input_scale_0', 'bert.encoder.layer.9.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.11.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.0.output.dense._packed_params._packed_params', 'bert.encoder.layer.2.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.4.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.1.attention.self.key.module.zero_point', 'bert.encoder.layer.9.attention.self.value.module_input_scale_0', 'bert.pooler.dense._packed_params._packed_params', 'bert.encoder.layer.5.output.dense.scale', 'bert.encoder.layer.3.output.LayerNorm.scale', 'bert.encoder.layer.5.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.8.attention.self.value.module_input_scale_0', 'bert.encoder.layer.5.attention.self.key.module_input_scale_0', 'bert.encoder.layer.5.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.3.intermediate.dense.zero_point', 'bert.encoder.layer.1.attention.self.query.module.zero_point', 'bert.encoder.layer.8.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.6.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.0.attention.output.dense.scale', 'bert.encoder.layer.1.intermediate.dense.zero_point', 'bert.encoder.layer.0.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.0.attention.self.value.module.scale', 'bert.encoder.layer.3.intermediate.dense.scale', 'bert.encoder.layer.8.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.7.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.9.intermediate.dense.zero_point', 'bert.encoder.layer.1.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.3.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.10.attention.output.dense.zero_point', 'bert.encoder.layer.1.output.dense._packed_params._packed_params', 'bert.encoder.layer.4.attention.self.key.module_input_scale_0', 'bert.encoder.layer.3.attention.output.dense_input_scale_0', 'bert.encoder.layer.8.attention.output.LayerNorm.scale', 'bert.encoder.layer.1.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.11.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.1.attention.self.query.module.scale', 'bert.encoder.layer.1.output.dense.zero_point', 'bert.encoder.layer.3.output.dense._packed_params._packed_params', 'bert.encoder.layer.10.attention.self.key.module.scale', 'bert.encoder.layer.0.attention.output.LayerNorm.scale', 'classifier.module.zero_point', 'bert.encoder.layer.3.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.4.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.7.intermediate.dense.zero_point', 'bert.encoder.layer.9.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.3.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.9.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.1.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.9.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.3.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.4.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.1.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.2.attention.self.key.module_input_zero_point_0', 'bert.embeddings.position_embeddings.module._packed_params._packed_weight', 'bert.encoder.layer.6.intermediate.dense.zero_point', 'bert.encoder.layer.11.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.3.output.dense_input_zero_point_0', 'bert.encoder.layer.5.attention.output.dense.scale', 'bert.encoder.layer.10.output.dense._packed_params.dtype', 'bert.encoder.layer.6.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.5.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.2.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.6.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.11.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.6.output.dense_input_zero_point_0', 'bert.encoder.layer.5.attention.self.query.module.scale', 'bert.encoder.layer.8.attention.self.key.module_input_scale_0', 'bert.encoder.layer.10.intermediate.dense_input_scale_0', 'bert.encoder.layer.6.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.8.attention.self.key.module.scale', 'bert.encoder.layer.4.attention.output.dense.scale', 'bert.encoder.layer.9.attention.self.query.module.zero_point', 'bert.encoder.layer.6.attention.self.key.module_input_scale_0', 'bert.encoder.layer.7.output.dense_input_scale_0', 'bert.encoder.layer.6.attention.self.query.module.zero_point', 'bert.encoder.layer.2.output.dense_input_scale_0', 'bert.encoder.layer.4.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.5.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.5.attention.output.dense.zero_point', 'classifier.module._packed_params.dtype', 'bert.encoder.layer.4.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.5.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.9.output.LayerNorm_input_scale_0', 'bert.encoder.layer.11.attention.self.value.module.scale', 'bert.encoder.layer.6.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.3.attention.self.key.module.zero_point', 'bert.encoder.layer.3.attention.self.query.module.scale', 'bert.encoder.layer.3.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.0.output.dense._packed_params.dtype', 'bert.encoder.layer.9.attention.self.key.module.scale', 'bert.encoder.layer.5.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.8.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.6.attention.self.value.module.scale', 'bert.encoder.layer.1.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.11.attention.output.LayerNorm.scale', 'bert.encoder.layer.4.attention.self.value.module.scale', 'bert.encoder.layer.8.attention.self.query.module.zero_point', 'bert.encoder.layer.3.attention.self.query.module_input_scale_0', 'bert.encoder.layer.0.attention.self.query.module.zero_point', 'bert.encoder.layer.1.output.dense._packed_params.dtype', 'bert.encoder.layer.6.output.dense.zero_point', 'bert.encoder.layer.10.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.1.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.7.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.4.attention.self.query.module_input_scale_0', 'bert.encoder.layer.5.attention.self.query.module_input_scale_0', 'bert.encoder.layer.6.output.dense._packed_params.dtype', 'bert.encoder.layer.10.attention.output.LayerNorm.scale', 'bert.encoder.layer.2.attention.self.value.module_input_zero_point_0', 'bert.embeddings.LayerNorm.module.zero_point', 'bert.encoder.layer.10.output.LayerNorm.zero_point', 'bert.encoder.layer.5.attention.self.query.module.zero_point', 'bert.encoder.layer.5.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.0.output.LayerNorm.scale', 'bert.encoder.layer.7.output.LayerNorm.scale', 'bert.encoder.layer.10.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.11.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.6.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.1.attention.output.dense.zero_point', 'bert.encoder.layer.9.output.dense.zero_point', 'bert.encoder.layer.8.attention.self.query.module._packed_params.dtype', 'classifier.module_input_zero_point_0', 'bert.encoder.layer.11.output.dense.zero_point', 'bert.encoder.layer.9.output.LayerNorm.zero_point', 'bert.encoder.layer.4.output.LayerNorm_input_scale_0', 'bert.encoder.layer.3.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.9.output.dense._packed_params._packed_params', 'bert.encoder.layer.4.output.dense_input_scale_0', 'classifier.module.scale', 'bert.embeddings.word_embeddings.module._packed_params._packed_weight', 'bert.encoder.layer.10.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.0.intermediate.dense.scale', 'bert.encoder.layer.8.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.0.attention.self.key.module.scale', 'bert.encoder.layer.7.attention.self.key.module_input_scale_0', 'bert.encoder.layer.4.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.0.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.3.attention.output.dense.scale', 'bert.encoder.layer.8.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.4.output.dense.scale', 'bert.encoder.layer.9.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.11.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.1.intermediate.dense.scale', 'bert.encoder.layer.1.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.9.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.2.attention.output.dense.scale', 'bert.encoder.layer.2.attention.self.value.module.scale', 'bert.encoder.layer.4.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.7.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.6.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.0.output.LayerNorm_input_scale_0', 'bert.encoder.layer.8.output.LayerNorm.zero_point', 'bert.encoder.layer.0.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.5.output.dense.zero_point', 'bert.encoder.layer.2.output.LayerNorm.zero_point', 'bert.encoder.layer.4.intermediate.dense.zero_point', 'bert.encoder.layer.7.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.9.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.8.output.dense._packed_params.dtype', 'bert.encoder.layer.3.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.6.output.LayerNorm.scale', 'bert.embeddings.word_embeddings.module._packed_params.dtype', 'bert.encoder.layer.4.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.7.output.dense.scale', 'bert.encoder.layer.9.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.11.attention.output.dense._packed_params._packed_params', 'bert.encoder.layer.11.attention.self.query.module_input_scale_0', 'bert.encoder.layer.11.attention.self.query.module.zero_point', 'bert.encoder.layer.6.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.10.attention.self.key.module.zero_point', 'bert.encoder.layer.8.attention.output.dense_input_scale_0', 'bert.encoder.layer.11.attention.output.dense_input_scale_0', 'bert.encoder.layer.0.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.5.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.8.output.dense_input_zero_point_0', 'bert.encoder.layer.0.intermediate.dense._packed_params._packed_params', 'bert.pooler.dense.zero_point', 'bert.encoder.layer.3.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.2.attention.self.key.module.scale', 'bert.encoder.layer.10.output.dense._packed_params._packed_params', 'bert.embeddings.token_type_embeddings.module._packed_params._packed_weight', 'bert.encoder.layer.2.output.dense.scale', 'bert.encoder.layer.1.output.LayerNorm.scale', 'bert.encoder.layer.9.attention.self.key.module.zero_point', 'bert.encoder.layer.2.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.1.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.3.attention.self.value.module_input_scale_0', 'bert.encoder.layer.5.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.10.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.11.output.dense._packed_params.dtype', 'bert.encoder.layer.7.attention.self.query.module.zero_point', 'bert.encoder.layer.2.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.1.attention.self.key.module.scale', 'bert.encoder.layer.5.output.dense_input_scale_0', 'bert.encoder.layer.10.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.1.attention.output.dense.scale', 'bert.encoder.layer.0.output.LayerNorm.zero_point', 'bert.encoder.layer.4.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.8.output.dense._packed_params._packed_params', 'bert.encoder.layer.4.output.dense.zero_point', 'bert.encoder.layer.4.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.4.output.dense_input_zero_point_0', 'bert.encoder.layer.8.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.11.attention.output.dense.scale', 'bert.embeddings.LayerNorm.module_input_scale_0', 'bert.encoder.layer.11.attention.self.query.module.scale', 'bert.encoder.layer.0.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.7.attention.output.dense.scale', 'bert.encoder.layer.10.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.10.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.5.output.dense._packed_params.dtype', 'bert.encoder.layer.5.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.11.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.3.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.3.intermediate.dense_input_scale_0', 'bert.encoder.layer.4.attention.self.query.module.zero_point', 'bert.encoder.layer.6.attention.self.query.module_input_scale_0', 'bert.encoder.layer.11.attention.self.key.module.scale', 'bert.encoder.layer.3.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.9.attention.output.LayerNorm.scale', 'bert.encoder.layer.11.output.dense_input_zero_point_0', 'bert.encoder.layer.4.intermediate.dense_input_scale_0', 'bert.encoder.layer.2.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.6.output.dense.scale', 'bert.encoder.layer.11.intermediate.dense_input_scale_0', 'bert.encoder.layer.7.attention.self.key.module.zero_point', 'bert.encoder.layer.6.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.10.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.8.attention.self.key.module.zero_point', 'bert.encoder.layer.6.output.dense_input_scale_0', 'bert.encoder.layer.10.attention.output.dense_input_scale_0', 'bert.encoder.layer.5.intermediate.dense_input_scale_0', 'bert.encoder.layer.6.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.3.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.5.attention.output.LayerNorm_input_scale_0', 'classifier.module._packed_params._packed_params', 'bert.encoder.layer.0.attention.self.value.module_input_scale_0', 'bert.encoder.layer.4.attention.output.dense.zero_point', 'bert.encoder.layer.5.output.LayerNorm.scale', 'bert.encoder.layer.3.attention.self.query.module.zero_point', 'bert.encoder.layer.7.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.0.output.dense.scale', 'bert.encoder.layer.5.intermediate.dense.scale', 'bert.encoder.layer.11.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.10.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.6.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.7.attention.self.query.module.scale', 'bert.encoder.layer.9.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.4.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.5.output.LayerNorm.zero_point', 'bert.encoder.layer.8.attention.self.query.module_input_scale_0', 'bert.encoder.layer.8.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.2.attention.output.LayerNorm.scale', 'bert.encoder.layer.9.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.6.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.7.output.dense._packed_params._packed_params', 'bert.encoder.layer.11.intermediate.dense.scale', 'bert.encoder.layer.0.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.4.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.8.intermediate.dense.scale', 'bert.encoder.layer.1.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.6.output.LayerNorm.zero_point', 'bert.encoder.layer.10.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.8.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.6.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.3.attention.self.key.module.scale', 'bert.encoder.layer.0.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.6.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.10.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.2.intermediate.dense.zero_point', 'bert.encoder.layer.10.output.dense_input_zero_point_0', 'bert.encoder.layer.9.output.dense_input_scale_0', 'bert.encoder.layer.3.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.6.attention.output.dense.zero_point', 'bert.encoder.layer.9.attention.self.query.module_input_scale_0', 'bert.encoder.layer.6.output.dense._packed_params._packed_params', 'bert.encoder.layer.10.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.2.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.7.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.9.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.11.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.1.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.1.output.LayerNorm_input_scale_0', 'bert.encoder.layer.3.output.dense.scale', 'bert.encoder.layer.4.attention.output.LayerNorm.scale', 'bert.embeddings.LayerNorm.module_input_zero_point_0', 'bert.encoder.layer.10.attention.self.query.module_input_scale_0', 'bert.encoder.layer.1.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.1.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.9.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.7.attention.self.query.module_input_scale_0', 'bert.encoder.layer.10.attention.self.query.module.zero_point', 'bert.encoder.layer.0.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.9.output.dense.scale', 'bert.encoder.layer.10.attention.self.key.module_input_scale_0', 'bert.encoder.layer.11.output.dense._packed_params._packed_params', 'bert.encoder.layer.11.output.LayerNorm_input_zero_point_0', 'bert.embeddings.LayerNorm.module.weight', 'bert.encoder.layer.8.output.LayerNorm_input_scale_0', 'bert.encoder.layer.4.output.dense._packed_params._packed_params', 'bert.encoder.layer.11.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.7.attention.self.value.module.zero_point', 'bert.encoder.layer.0.attention.self.key.module.zero_point', 'bert.encoder.layer.1.attention.self.value.module.zero_point', 'bert.encoder.layer.7.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.1.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.8.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.7.attention.self.key.module.scale', 'bert.encoder.layer.7.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.0.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.7.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.1.intermediate.dense_input_scale_0', 'bert.encoder.layer.5.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.5.attention.self.key.module.scale', 'bert.encoder.layer.11.output.LayerNorm.scale', 'bert.encoder.layer.11.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.5.intermediate.dense._packed_params._packed_params', 'bert.encoder.layer.10.attention.output.dense.scale', 'bert.encoder.layer.9.attention.output.dense_input_scale_0', 'bert.encoder.layer.2.output.dense.zero_point', 'bert.encoder.layer.10.attention.self.value.module.scale', 'bert.encoder.layer.4.output.LayerNorm.scale', 'bert.embeddings.LayerNorm.module.scale', 'bert.encoder.layer.2.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.5.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.1.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.10.attention.self.value.module_input_scale_0', 'bert.encoder.layer.4.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.2.output.dense._packed_params.dtype', 'best_configure', 'bert.encoder.layer.10.intermediate.dense.zero_point', 'bert.encoder.layer.9.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.0.attention.self.key.module_input_scale_0', 'bert.encoder.layer.8.attention.self.query.module.scale', 'bert.encoder.layer.11.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.9.attention.self.value.module.scale', 'bert.pooler.dense_input_zero_point_0', 'bert.pooler.dense.scale', 'bert.encoder.layer.9.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.0.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.1.attention.self.query.module_input_scale_0', 'bert.encoder.layer.10.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.8.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.9.attention.output.dense.zero_point', 'bert.encoder.layer.0.intermediate.dense_input_scale_0', 'bert.encoder.layer.7.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.9.output.LayerNorm.scale', 'bert.encoder.layer.3.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.8.intermediate.dense.zero_point', 'bert.embeddings.LayerNorm.module.bias', 'bert.encoder.layer.6.attention.output.dense.scale', 'bert.encoder.layer.7.output.dense.zero_point', 'bert.encoder.layer.11.attention.output.dense._packed_params.dtype', 'bert.encoder.layer.6.output.LayerNorm_input_scale_0', 'bert.encoder.layer.6.attention.output.dense_input_scale_0', 'bert.encoder.layer.10.attention.self.value.module_input_zero_point_0', 'bert.encoder.layer.10.output.LayerNorm.scale', 'bert.encoder.layer.2.attention.self.query.module_input_scale_0', 'bert.encoder.layer.1.attention.output.dense_input_scale_0', 'bert.encoder.layer.7.attention.self.value.module_input_scale_0', 'bert.encoder.layer.8.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.4.intermediate.dense.scale', 'bert.encoder.layer.2.output.LayerNorm_input_scale_0', 'bert.encoder.layer.10.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.10.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.6.attention.output.LayerNorm.scale', 'bert.encoder.layer.3.attention.self.value.module.scale', 'bert.encoder.layer.1.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.11.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.6.intermediate.dense.scale', 'bert.encoder.layer.6.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.3.attention.output.dense.zero_point', 'bert.encoder.layer.7.attention.self.value.module.scale', 'bert.encoder.layer.0.output.dense.zero_point', 'bert.encoder.layer.1.output.dense_input_scale_0', 'bert.encoder.layer.8.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.2.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.8.output.dense.scale', 'bert.encoder.layer.5.attention.output.LayerNorm.scale', 'bert.encoder.layer.9.attention.self.key.module_input_scale_0', 'bert.encoder.layer.1.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.9.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.3.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.10.attention.self.value.module.zero_point', 'bert.encoder.layer.3.attention.self.key.module_input_scale_0', 'bert.encoder.layer.3.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.10.intermediate.dense.scale', 'bert.encoder.layer.9.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.5.output.dense_input_zero_point_0', 'bert.encoder.layer.9.output.dense._packed_params.dtype', 'bert.encoder.layer.0.intermediate.dense.zero_point', 'bert.encoder.layer.1.output.LayerNorm.zero_point', 'bert.encoder.layer.5.intermediate.dense.zero_point', 'bert.encoder.layer.2.attention.output.LayerNorm.zero_point', 'bert.encoder.layer.11.attention.self.value.module_input_scale_0', 'bert.encoder.layer.7.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.0.output.dense_input_scale_0', 'bert.encoder.layer.1.attention.self.value.module.scale', 'bert.encoder.layer.2.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.3.output.dense._packed_params.dtype', 'bert.encoder.layer.7.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.7.attention.output.dense_input_scale_0', 'bert.encoder.layer.7.output.LayerNorm_input_scale_0', 'bert.encoder.layer.9.output.dense_input_zero_point_0', 'bert.encoder.layer.3.output.LayerNorm_input_scale_0', 'bert.encoder.layer.4.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.1.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.3.output.dense.zero_point', 'bert.encoder.layer.0.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.0.attention.self.query.module.scale', 'bert.encoder.layer.8.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.4.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.2.attention.self.key.module_input_scale_0', 'bert.encoder.layer.2.attention.output.dense.zero_point', 'bert.encoder.layer.2.intermediate.dense.scale', 'bert.encoder.layer.5.attention.self.value.module.zero_point', 'bert.encoder.layer.11.output.dense.scale', 'bert.encoder.layer.2.intermediate.dense_input_zero_point_0', 'bert.encoder.layer.3.attention.output.LayerNorm.scale', 'bert.encoder.layer.3.attention.self.key.module._packed_params._packed_params', 'bert.encoder.layer.6.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.10.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.2.output.dense_input_zero_point_0', 'bert.encoder.layer.10.attention.self.value.module._packed_params._packed_params', 'bert.encoder.layer.7.intermediate.dense_input_scale_0', 'bert.encoder.layer.0.attention.output.LayerNorm_input_zero_point_0', 'bert.encoder.layer.6.attention.self.query.module._packed_params.dtype', 'bert.encoder.layer.11.attention.self.value.module.zero_point', 'bert.encoder.layer.5.attention.self.query.module_input_zero_point_0', 'bert.encoder.layer.0.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.11.intermediate.dense.zero_point', 'classifier.module_input_scale_0', 'bert.encoder.layer.1.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.11.output.dense_input_scale_0', 'bert.encoder.layer.4.intermediate.dense._packed_params.dtype', 'bert.encoder.layer.8.output.dense.zero_point', 'bert.encoder.layer.5.attention.self.value.module_input_scale_0', 'bert.encoder.layer.1.attention.output.LayerNorm.scale', 'bert.encoder.layer.6.attention.self.key.module.zero_point', 'bert.encoder.layer.6.intermediate.dense_input_scale_0', 'bert.encoder.layer.7.intermediate.dense.scale', 'bert.encoder.layer.10.output.LayerNorm_input_scale_0', 'bert.encoder.layer.4.attention.self.value.module.zero_point', 'bert.encoder.layer.6.attention.self.query.module.scale', 'bert.encoder.layer.2.attention.self.query.module._packed_params._packed_params', 'bert.encoder.layer.7.output.LayerNorm.zero_point', 'bert.encoder.layer.0.attention.output.dense_input_scale_0', 'bert.encoder.layer.7.output.dense_input_zero_point_0', 'bert.pooler.dense_input_scale_0', 'bert.encoder.layer.7.attention.self.key.module._packed_params.dtype', 'bert.encoder.layer.6.attention.self.value.module._packed_params.dtype', 'bert.encoder.layer.8.attention.output.LayerNorm_input_scale_0', 'bert.encoder.layer.11.attention.self.key.module.zero_point', 'bert.encoder.layer.2.output.dense._packed_params._packed_params', 'bert.encoder.layer.0.attention.output.dense_input_zero_point_0', 'bert.encoder.layer.6.attention.self.value.module.zero_point', 'bert.encoder.layer.11.attention.self.key.module_input_zero_point_0', 'bert.encoder.layer.1.output.dense_input_zero_point_0', 'bert.encoder.layer.9.attention.self.key.module._packed_params._packed_params']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3777] 2023-11-10 18:54:26,005 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../saved_results and are newly initialized: ['bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'classifier.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/10/2023 18:54:26 - WARNING - __main__ - Your model seems to have been trained with labels, but they don't match the dataset: model labels: ['0', '1'], dataset labels: ['equivalent', 'not_equivalent'].
Ignoring the model labels as a result.
/home/21js160/qat_bert/export_model/./run_glue.py:457: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("glue", data_args.task_name)
2023-11-10 18:54:28 [INFO] Start auto tuning.
2023-11-10 18:54:28 [INFO] Execute the tuning process due to detect the evaluation function.
2023-11-10 18:54:28 [INFO] Adaptor has 5 recipes.
2023-11-10 18:54:28 [INFO] 0 recipes specified by user.
2023-11-10 18:54:28 [INFO] 3 recipes require future tuning.
2023-11-10 18:54:28 [INFO] *** Initialize auto tuning
2023-11-10 18:54:28 [INFO] {
2023-11-10 18:54:28 [INFO]     'PostTrainingQuantConfig': {
2023-11-10 18:54:28 [INFO]         'AccuracyCriterion': {
2023-11-10 18:54:28 [INFO]             'criterion': 'relative',
2023-11-10 18:54:28 [INFO]             'higher_is_better': True,
2023-11-10 18:54:28 [INFO]             'tolerable_loss': 0.01,
2023-11-10 18:54:28 [INFO]             'absolute': None,
2023-11-10 18:54:28 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f0b975bc9d0>>,
2023-11-10 18:54:28 [INFO]             'relative': 0.01
2023-11-10 18:54:28 [INFO]         },
2023-11-10 18:54:28 [INFO]         'approach': 'post_training_dynamic_quant',
2023-11-10 18:54:28 [INFO]         'backend': 'default',
2023-11-10 18:54:28 [INFO]         'calibration_sampling_size': [
2023-11-10 18:54:28 [INFO]             100
2023-11-10 18:54:28 [INFO]         ],
2023-11-10 18:54:28 [INFO]         'device': 'cpu',
2023-11-10 18:54:28 [INFO]         'diagnosis': False,
2023-11-10 18:54:28 [INFO]         'domain': 'auto',
2023-11-10 18:54:28 [INFO]         'example_inputs': None,
2023-11-10 18:54:28 [INFO]         'excluded_precisions': [
2023-11-10 18:54:28 [INFO]         ],
2023-11-10 18:54:28 [INFO]         'framework': 'pytorch_fx',
2023-11-10 18:54:28 [INFO]         'inputs': [
2023-11-10 18:54:28 [INFO]         ],
2023-11-10 18:54:28 [INFO]         'model_name': '',
2023-11-10 18:54:28 [INFO]         'ni_workload_name': 'quantization',
2023-11-10 18:54:28 [INFO]         'op_name_dict': None,
2023-11-10 18:54:28 [INFO]         'op_type_dict': None,
2023-11-10 18:54:28 [INFO]         'outputs': [
2023-11-10 18:54:28 [INFO]         ],
2023-11-10 18:54:28 [INFO]         'quant_format': 'default',
2023-11-10 18:54:28 [INFO]         'quant_level': 'auto',
2023-11-10 18:54:28 [INFO]         'recipes': {
2023-11-10 18:54:28 [INFO]             'smooth_quant': False,
2023-11-10 18:54:28 [INFO]             'smooth_quant_args': {
2023-11-10 18:54:28 [INFO]             },
2023-11-10 18:54:28 [INFO]             'layer_wise_quant': False,
2023-11-10 18:54:28 [INFO]             'layer_wise_quant_args': {
2023-11-10 18:54:28 [INFO]             },
2023-11-10 18:54:28 [INFO]             'fast_bias_correction': False,
2023-11-10 18:54:28 [INFO]             'weight_correction': False,
2023-11-10 18:54:28 [INFO]             'gemm_to_matmul': True,
2023-11-10 18:54:28 [INFO]             'graph_optimization_level': None,
2023-11-10 18:54:28 [INFO]             'first_conv_or_matmul_quantization': True,
2023-11-10 18:54:28 [INFO]             'last_conv_or_matmul_quantization': True,
2023-11-10 18:54:28 [INFO]             'pre_post_process_quantization': True,
2023-11-10 18:54:28 [INFO]             'add_qdq_pair_to_weight': False,
2023-11-10 18:54:28 [INFO]             'optypes_to_exclude_output_quant': [
2023-11-10 18:54:28 [INFO]             ],
2023-11-10 18:54:28 [INFO]             'dedicated_qdq_pair': False,
2023-11-10 18:54:28 [INFO]             'rtn_args': {
2023-11-10 18:54:28 [INFO]             },
2023-11-10 18:54:28 [INFO]             'awq_args': {
2023-11-10 18:54:28 [INFO]             },
2023-11-10 18:54:28 [INFO]             'gptq_args': {
2023-11-10 18:54:28 [INFO]             },
2023-11-10 18:54:28 [INFO]             'teq_args': {
2023-11-10 18:54:28 [INFO]             }
2023-11-10 18:54:28 [INFO]         },
2023-11-10 18:54:28 [INFO]         'reduce_range': None,
2023-11-10 18:54:28 [INFO]         'TuningCriterion': {
2023-11-10 18:54:28 [INFO]             'max_trials': 100,
2023-11-10 18:54:28 [INFO]             'objective': [
2023-11-10 18:54:28 [INFO]                 'performance'
2023-11-10 18:54:28 [INFO]             ],
2023-11-10 18:54:28 [INFO]             'strategy': 'basic',
2023-11-10 18:54:28 [INFO]             'strategy_kwargs': None,
2023-11-10 18:54:28 [INFO]             'timeout': 0
2023-11-10 18:54:28 [INFO]         },
2023-11-10 18:54:28 [INFO]         'use_bf16': True
2023-11-10 18:54:28 [INFO]     }
2023-11-10 18:54:28 [INFO] }
2023-11-10 18:54:28 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.
2023-11-10 18:54:28 [INFO]  Found 12 blocks
2023-11-10 18:54:28 [INFO] Attention Blocks: 12
2023-11-10 18:54:28 [INFO] FFN Blocks: 12
2023-11-10 18:54:28 [INFO] Pass query framework capability elapsed time: 361.52 ms
2023-11-10 18:54:28 [INFO] Get FP32 model baseline.
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:02,  8.39it/s] 12%|█▏        | 3/26 [00:00<00:03,  6.01it/s] 15%|█▌        | 4/26 [00:00<00:03,  5.77it/s] 19%|█▉        | 5/26 [00:00<00:04,  5.05it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.96it/s] 27%|██▋       | 7/26 [00:01<00:03,  4.89it/s] 31%|███       | 8/26 [00:01<00:03,  4.93it/s] 35%|███▍      | 9/26 [00:01<00:03,  5.05it/s] 38%|███▊      | 10/26 [00:01<00:03,  4.53it/s] 42%|████▏     | 11/26 [00:02<00:03,  4.29it/s] 46%|████▌     | 12/26 [00:02<00:03,  3.95it/s] 50%|█████     | 13/26 [00:02<00:03,  4.14it/s] 54%|█████▍    | 14/26 [00:02<00:02,  4.19it/s] 58%|█████▊    | 15/26 [00:03<00:02,  4.36it/s] 62%|██████▏   | 16/26 [00:03<00:02,  4.60it/s] 65%|██████▌   | 17/26 [00:03<00:01,  4.60it/s] 69%|██████▉   | 18/26 [00:03<00:01,  4.80it/s] 73%|███████▎  | 19/26 [00:04<00:01,  4.70it/s] 77%|███████▋  | 20/26 [00:04<00:01,  4.70it/s] 81%|████████  | 21/26 [00:04<00:01,  3.13it/s] 85%|████████▍ | 22/26 [00:05<00:01,  2.81it/s] 88%|████████▊ | 23/26 [00:05<00:00,  3.13it/s] 92%|█████████▏| 24/26 [00:05<00:00,  3.36it/s] 96%|█████████▌| 25/26 [00:05<00:00,  3.61it/s]100%|██████████| 26/26 [00:06<00:00,  4.37it/s]wandb: Currently logged in as: jkrt-ngh69 (abc_12). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/21js160/qat_bert/export_model/wandb/run-20231110_185438-6q9qxm0p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-spaceship-361
wandb: ⭐️ View project at https://wandb.ai/abc_12/huggingface
wandb: 🚀 View run at https://wandb.ai/abc_12/huggingface/runs/6q9qxm0p
100%|██████████| 26/26 [00:17<00:00,  1.46it/s]
Batch size = 16
Finally Eval eval_f1 Accuracy: 0.014184397163120569
Latency: 15.662 ms
Throughput: 63.849 samples/sec
2023-11-10 18:54:46 [INFO] Save tuning history to /home/21js160/qat_bert/export_model/nc_workspace/2023-11-10_18-54-26/./history.snapshot.
2023-11-10 18:54:46 [INFO] FP32 baseline is: [Accuracy: 0.0142, Duration (seconds): 18.1811]
2023-11-10 18:54:46 [INFO] Quantize the model with default config.
2023-11-10 18:54:47 [INFO] Fx trace of the entire model failed, We will conduct auto quantization
2023-11-10 18:54:49 [INFO] |******Mixed Precision Statistics******|
2023-11-10 18:54:49 [INFO] +-----------------+----------+---------+
2023-11-10 18:54:49 [INFO] |     Op Type     |  Total   |   INT8  |
2023-11-10 18:54:49 [INFO] +-----------------+----------+---------+
2023-11-10 18:54:49 [INFO] |    Embedding    |    3     |    3    |
2023-11-10 18:54:49 [INFO] |      Linear     |    74    |    74   |
2023-11-10 18:54:49 [INFO] +-----------------+----------+---------+
2023-11-10 18:54:49 [INFO] Pass quantize model elapsed time: 2873.56 ms
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:02, 10.33it/s] 15%|█▌        | 4/26 [00:00<00:03,  6.64it/s] 19%|█▉        | 5/26 [00:00<00:03,  6.03it/s] 23%|██▎       | 6/26 [00:00<00:03,  5.82it/s] 27%|██▋       | 7/26 [00:01<00:03,  5.71it/s] 31%|███       | 8/26 [00:01<00:03,  5.66it/s] 35%|███▍      | 9/26 [00:01<00:03,  5.52it/s] 38%|███▊      | 10/26 [00:01<00:02,  5.51it/s] 42%|████▏     | 11/26 [00:01<00:02,  5.52it/s] 46%|████▌     | 12/26 [00:02<00:02,  5.08it/s] 50%|█████     | 13/26 [00:02<00:02,  5.07it/s] 54%|█████▍    | 14/26 [00:02<00:02,  4.90it/s] 58%|█████▊    | 15/26 [00:02<00:02,  4.96it/s] 62%|██████▏   | 16/26 [00:02<00:01,  5.05it/s] 65%|██████▌   | 17/26 [00:03<00:01,  5.12it/s] 69%|██████▉   | 18/26 [00:03<00:01,  5.17it/s] 73%|███████▎  | 19/26 [00:03<00:01,  5.15it/s] 77%|███████▋  | 20/26 [00:03<00:01,  5.26it/s] 81%|████████  | 21/26 [00:03<00:00,  5.21it/s] 85%|████████▍ | 22/26 [00:04<00:00,  5.08it/s] 88%|████████▊ | 23/26 [00:04<00:00,  5.16it/s] 92%|█████████▏| 24/26 [00:04<00:00,  5.20it/s] 96%|█████████▌| 25/26 [00:04<00:00,  5.30it/s]100%|██████████| 26/26 [00:04<00:00,  6.11it/s]100%|██████████| 26/26 [00:04<00:00,  5.48it/s]
Batch size = 16
Finally Eval eval_f1 Accuracy: 0.42512077294685985
Latency: 12.224 ms
Throughput: 81.804 samples/sec
2023-11-10 18:54:54 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.4251|0.0142, Duration (seconds) (int8|fp32): 4.9910|18.1811], Best tune result is: [Accuracy: 0.4251, Duration (seconds): 4.9910]
2023-11-10 18:54:54 [INFO] |**********************Tune Result Statistics**********************|
2023-11-10 18:54:54 [INFO] +--------------------+----------+---------------+------------------+
2023-11-10 18:54:54 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2023-11-10 18:54:54 [INFO] +--------------------+----------+---------------+------------------+
2023-11-10 18:54:54 [INFO] |      Accuracy      | 0.0142   |    0.4251     |     0.4251       |
2023-11-10 18:54:54 [INFO] | Duration (seconds) | 18.1811  |    4.9910     |     4.9910       |
2023-11-10 18:54:54 [INFO] +--------------------+----------+---------------+------------------+
2023-11-10 18:54:54 [INFO] [Strategy] Found a model that meets the accuracy requirements.
2023-11-10 18:54:54 [INFO] Save tuning history to /home/21js160/qat_bert/export_model/nc_workspace/2023-11-10_18-54-26/./history.snapshot.
2023-11-10 18:54:54 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.
2023-11-10 18:54:54 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2023-11-10 18:54:54 [INFO] Save deploy yaml to /home/21js160/qat_bert/export_model/nc_workspace/2023-11-10_18-54-26/deploy.yaml
2023-11-10 18:55:00 [INFO] Quantization format is not available when executing dynamic quantization.
11/10/2023 18:55:03 - WARNING - root - Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md 
2023-11-10 18:55:24 [INFO] **************************************************************************
2023-11-10 18:55:24 [INFO] The INT8 ONNX Model exported to path: bert-base-uncased-mrpc-int8-qat.onnx
2023-11-10 18:55:24 [INFO] **************************************************************************
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.049 MB uploaded (0.000 MB deduped)wandb: | 0.023 MB of 0.049 MB uploaded (0.000 MB deduped)wandb: / 0.023 MB of 0.049 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:           eval/accuracy ▁█
wandb:     eval/combined_score ▁█
wandb:                 eval/f1 ▁█
wandb:               eval/loss █▁
wandb:            eval/runtime █▁
wandb: eval/samples_per_second ▁█
wandb:   eval/steps_per_second ▁█
wandb:       train/global_step ▁▁
wandb: 
wandb: Run summary:
wandb:           eval/accuracy 0.41667
wandb:     eval/combined_score 0.42089
wandb:                 eval/f1 0.42512
wandb:               eval/loss 0.69445
wandb:            eval/runtime 4.9876
wandb: eval/samples_per_second 81.804
wandb:   eval/steps_per_second 5.213
wandb:       train/global_step 0
wandb: 
wandb: 🚀 View run autumn-spaceship-361 at: https://wandb.ai/abc_12/huggingface/runs/6q9qxm0p
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231110_185438-6q9qxm0p/logs
