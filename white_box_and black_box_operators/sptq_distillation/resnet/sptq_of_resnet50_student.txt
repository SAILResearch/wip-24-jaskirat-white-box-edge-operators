nohup: ignoring input
2023-09-26 01:22:57 [INFO] Start auto tuning.
2023-09-26 01:22:57 [INFO] Execute the tuning process due to detect the evaluation function.
2023-09-26 01:22:57 [INFO] Adaptor has 5 recipes.
2023-09-26 01:22:57 [INFO] 0 recipes specified by user.
2023-09-26 01:22:57 [INFO] 3 recipes require future tuning.
2023-09-26 01:22:57 [INFO] *** Initialize auto tuning
2023-09-26 01:22:57 [INFO] {
2023-09-26 01:22:57 [INFO]     'PostTrainingQuantConfig': {
2023-09-26 01:22:57 [INFO]         'AccuracyCriterion': {
2023-09-26 01:22:57 [INFO]             'criterion': 'relative',
2023-09-26 01:22:57 [INFO]             'higher_is_better': True,
2023-09-26 01:22:57 [INFO]             'tolerable_loss': 0.01,
2023-09-26 01:22:57 [INFO]             'absolute': None,
2023-09-26 01:22:57 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7fa600294490>>,
2023-09-26 01:22:57 [INFO]             'relative': 0.01
2023-09-26 01:22:57 [INFO]         },
2023-09-26 01:22:57 [INFO]         'approach': 'post_training_static_quant',
2023-09-26 01:22:57 [INFO]         'backend': 'default',
2023-09-26 01:22:57 [INFO]         'calibration_sampling_size': [
2023-09-26 01:22:57 [INFO]             100
2023-09-26 01:22:57 [INFO]         ],
2023-09-26 01:22:57 [INFO]         'device': 'cpu',
2023-09-26 01:22:57 [INFO]         'diagnosis': False,
2023-09-26 01:22:57 [INFO]         'domain': 'auto',
2023-09-26 01:22:57 [INFO]         'example_inputs': None,
2023-09-26 01:22:57 [INFO]         'excluded_precisions': [
2023-09-26 01:22:57 [INFO]         ],
2023-09-26 01:22:57 [INFO]         'framework': 'pytorch_fx',
2023-09-26 01:22:57 [INFO]         'inputs': [
2023-09-26 01:22:57 [INFO]         ],
2023-09-26 01:22:57 [INFO]         'model_name': '',
2023-09-26 01:22:57 [INFO]         'ni_workload_name': 'quantization',
2023-09-26 01:22:57 [INFO]         'op_name_dict': None,
2023-09-26 01:22:57 [INFO]         'op_type_dict': None,
2023-09-26 01:22:57 [INFO]         'outputs': [
2023-09-26 01:22:57 [INFO]         ],
2023-09-26 01:22:57 [INFO]         'quant_format': 'default',
2023-09-26 01:22:57 [INFO]         'quant_level': 'auto',
2023-09-26 01:22:57 [INFO]         'recipes': {
2023-09-26 01:22:57 [INFO]             'smooth_quant': False,
2023-09-26 01:22:57 [INFO]             'smooth_quant_args': {
2023-09-26 01:22:57 [INFO]             },
2023-09-26 01:22:57 [INFO]             'layer_wise_quant': False,
2023-09-26 01:22:57 [INFO]             'layer_wise_quant_args': {
2023-09-26 01:22:57 [INFO]             },
2023-09-26 01:22:57 [INFO]             'fast_bias_correction': False,
2023-09-26 01:22:57 [INFO]             'weight_correction': False,
2023-09-26 01:22:57 [INFO]             'gemm_to_matmul': True,
2023-09-26 01:22:57 [INFO]             'graph_optimization_level': None,
2023-09-26 01:22:57 [INFO]             'first_conv_or_matmul_quantization': True,
2023-09-26 01:22:57 [INFO]             'last_conv_or_matmul_quantization': True,
2023-09-26 01:22:57 [INFO]             'pre_post_process_quantization': True,
2023-09-26 01:22:57 [INFO]             'add_qdq_pair_to_weight': False,
2023-09-26 01:22:57 [INFO]             'optypes_to_exclude_output_quant': [
2023-09-26 01:22:57 [INFO]             ],
2023-09-26 01:22:57 [INFO]             'dedicated_qdq_pair': False,
2023-09-26 01:22:57 [INFO]             'rtn_args': {
2023-09-26 01:22:57 [INFO]             },
2023-09-26 01:22:57 [INFO]             'awq_args': {
2023-09-26 01:22:57 [INFO]             },
2023-09-26 01:22:57 [INFO]             'gptq_args': {
2023-09-26 01:22:57 [INFO]             },
2023-09-26 01:22:57 [INFO]             'teq_args': {
2023-09-26 01:22:57 [INFO]             }
2023-09-26 01:22:57 [INFO]         },
2023-09-26 01:22:57 [INFO]         'reduce_range': None,
2023-09-26 01:22:57 [INFO]         'TuningCriterion': {
2023-09-26 01:22:57 [INFO]             'max_trials': 100,
2023-09-26 01:22:57 [INFO]             'objective': [
2023-09-26 01:22:57 [INFO]                 'performance'
2023-09-26 01:22:57 [INFO]             ],
2023-09-26 01:22:57 [INFO]             'strategy': 'basic',
2023-09-26 01:22:57 [INFO]             'strategy_kwargs': None,
2023-09-26 01:22:57 [INFO]             'timeout': 0
2023-09-26 01:22:57 [INFO]         },
2023-09-26 01:22:57 [INFO]         'use_bf16': True
2023-09-26 01:22:57 [INFO]     }
2023-09-26 01:22:57 [INFO] }
2023-09-26 01:22:57 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.
/home/21js160/qat_roberta/env/lib/python3.10/site-packages/torch/ao/quantization/fx/fuse.py:54: UserWarning: Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.
  warnings.warn(
2023-09-26 01:23:00 [INFO] Attention Blocks: 0
2023-09-26 01:23:00 [INFO] FFN Blocks: 0
2023-09-26 01:23:00 [INFO] Pass query framework capability elapsed time: 2927.64 ms
2023-09-26 01:23:00 [INFO] Get FP32 model baseline.
=> creating model 'wide_resnet50_2'
2023-09-26 02:57:09 [INFO] Save tuning history to /home/21js160/white_box_operations_for_image_models/nc_workspace/2023-09-26_01-22-53/./history.snapshot.
2023-09-26 02:57:09 [INFO] FP32 baseline is: [Accuracy: 0.8140, Duration (seconds): 5649.4994]
2023-09-26 02:57:09 [INFO] Quantize the model with default config.
/home/21js160/qat_roberta/env/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
2023-09-26 02:57:12 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 256. So the real sampling size is 256.
Test: [  0/196]	Time  0.000 ( 0.000)	Loss 3.4048e-01 (3.4048e-01)	Acc@1  91.41 ( 91.41)	Acc@5  97.66 ( 97.66)
Test: [ 10/196]	Time 28.797 (28.337)	Loss 7.6198e-01 (5.0050e-01)	Acc@1  82.03 ( 87.18)	Acc@5  96.48 ( 97.76)
Test: [ 20/196]	Time 27.847 (27.566)	Loss 4.9989e-01 (5.1043e-01)	Acc@1  91.41 ( 86.79)	Acc@5  95.31 ( 97.49)
Test: [ 30/196]	Time 27.401 (27.605)	Loss 6.0497e-01 (4.8178e-01)	Acc@1  85.16 ( 87.61)	Acc@5  96.88 ( 97.72)
Test: [ 40/196]	Time 26.341 (27.297)	Loss 5.5021e-01 (5.2809e-01)	Acc@1  87.50 ( 86.50)	Acc@5  96.88 ( 97.53)
Test: [ 50/196]	Time 28.484 (27.517)	Loss 3.0130e-01 (5.3124e-01)	Acc@1  92.19 ( 86.34)	Acc@5  98.44 ( 97.52)
Test: [ 60/196]	Time 28.443 (27.507)	Loss 6.5833e-01 (5.5012e-01)	Acc@1  82.03 ( 85.90)	Acc@5  96.88 ( 97.46)
Test: [ 70/196]	Time 27.846 (27.528)	Loss 6.3024e-01 (5.3571e-01)	Acc@1  82.42 ( 86.12)	Acc@5  98.83 ( 97.60)
Test: [ 80/196]	Time 27.379 (27.486)	Loss 9.8794e-01 (5.5506e-01)	Acc@1  75.78 ( 85.70)	Acc@5  93.36 ( 97.41)
Test: [ 90/196]	Time 28.494 (27.606)	Loss 1.6101e+00 (5.9366e-01)	Acc@1  63.67 ( 84.76)	Acc@5  89.84 ( 97.03)
Test: [100/196]	Time 29.985 (27.777)	Loss 8.1790e-01 (6.3073e-01)	Acc@1  76.95 ( 83.83)	Acc@5  95.70 ( 96.70)
Test: [110/196]	Time 29.968 (27.949)	Loss 6.1872e-01 (6.4749e-01)	Acc@1  83.98 ( 83.51)	Acc@5  96.88 ( 96.55)
Test: [120/196]	Time 30.116 (28.081)	Loss 1.0281e+00 (6.5859e-01)	Acc@1  76.95 ( 83.44)	Acc@5  92.97 ( 96.37)
Test: [130/196]	Time 30.188 (28.192)	Loss 4.4323e-01 (6.8319e-01)	Acc@1  85.16 ( 82.78)	Acc@5  98.44 ( 96.11)
Test: [140/196]	Time 30.010 (28.354)	Loss 7.2912e-01 (6.9467e-01)	Acc@1  83.20 ( 82.57)	Acc@5  96.48 ( 95.99)
Test: [150/196]	Time 30.054 (28.525)	Loss 7.3285e-01 (7.0979e-01)	Acc@1  84.77 ( 82.25)	Acc@5  94.92 ( 95.78)
Test: [160/196]	Time 30.137 (28.613)	Loss 5.1041e-01 (7.1954e-01)	Acc@1  87.11 ( 82.07)	Acc@5  96.09 ( 95.66)
Test: [170/196]	Time 28.925 (28.687)	Loss 4.2985e-01 (7.3172e-01)	Acc@1  87.50 ( 81.75)	Acc@5  98.44 ( 95.56)
Test: [180/196]	Time 29.947 (28.746)	Loss 1.0297e+00 (7.4545e-01)	Acc@1  73.44 ( 81.40)	Acc@5  95.70 ( 95.45)
Test: [190/196]	Time 29.788 (28.819)	Loss 1.1390e+00 (7.4699e-01)	Acc@1  71.88 ( 81.33)	Acc@5  94.92 ( 95.48)
Batch size = 256
Accuracy: 0.81404 Accuracy@5 0.95510
2023-09-26 02:57:59 [INFO] |******Mixed Precision Statistics******|
2023-09-26 02:57:59 [INFO] +----------------------+-------+-------+
2023-09-26 02:57:59 [INFO] |       Op Type        | Total |  INT8 |
2023-09-26 02:57:59 [INFO] +----------------------+-------+-------+
2023-09-26 02:57:59 [INFO] | quantize_per_tensor  |   1   |   1   |
2023-09-26 02:57:59 [INFO] |      ConvReLU2d      |   33  |   33  |
2023-09-26 02:57:59 [INFO] |      MaxPool2d       |   1   |   1   |
2023-09-26 02:57:59 [INFO] |        Conv2d        |   20  |   20  |
2023-09-26 02:57:59 [INFO] |       add_relu       |   16  |   16  |
2023-09-26 02:57:59 [INFO] |  AdaptiveAvgPool2d   |   1   |   1   |
2023-09-26 02:57:59 [INFO] |       flatten        |   1   |   1   |
2023-09-26 02:57:59 [INFO] |        Linear        |   1   |   1   |
2023-09-26 02:57:59 [INFO] |      dequantize      |   1   |   1   |
2023-09-26 02:57:59 [INFO] +----------------------+-------+-------+
2023-09-26 02:57:59 [INFO] Pass quantize model elapsed time: 49355.98 ms
2023-09-26 03:11:52 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.8108|0.8140, Duration (seconds) (int8|fp32): 833.0743|5649.4994], Best tune result is: [Accuracy: 0.8108, Duration (seconds): 833.0743]
2023-09-26 03:11:52 [INFO] |***********************Tune Result Statistics***********************|
2023-09-26 03:11:52 [INFO] +--------------------+------------+---------------+------------------+
2023-09-26 03:11:52 [INFO] |     Info Type      |  Baseline  | Tune 1 result | Best tune result |
2023-09-26 03:11:52 [INFO] +--------------------+------------+---------------+------------------+
2023-09-26 03:11:52 [INFO] |      Accuracy      |  0.8140    |    0.8108     |     0.8108       |
2023-09-26 03:11:52 [INFO] | Duration (seconds) | 5649.4994  |   833.0743    |    833.0743      |
2023-09-26 03:11:52 [INFO] +--------------------+------------+---------------+------------------+
2023-09-26 03:11:52 [INFO] [Strategy] Found a model that meets the accuracy requirements.
2023-09-26 03:11:52 [INFO] Save tuning history to /home/21js160/white_box_operations_for_image_models/nc_workspace/2023-09-26_01-22-53/./history.snapshot.
2023-09-26 03:11:52 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.
2023-09-26 03:11:52 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2023-09-26 03:11:52 [INFO] Save deploy yaml to /home/21js160/white_box_operations_for_image_models/nc_workspace/2023-09-26_01-22-53/deploy.yaml
2023-09-26 03:11:52 [INFO] Save config file and weights of quantized model to /home/21js160/white_box_operations_for_image_models/sptq_of_resnet50_distilled_model.
Test: [  0/196]	Time  0.000 ( 0.000)	Loss 3.4776e-01 (3.4776e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.83 ( 98.83)
Test: [ 10/196]	Time  4.180 ( 4.319)	Loss 7.4634e-01 (5.0797e-01)	Acc@1  81.64 ( 86.72)	Acc@5  96.48 ( 97.76)
Test: [ 20/196]	Time  4.099 ( 4.368)	Loss 4.9810e-01 (5.1834e-01)	Acc@1  91.80 ( 86.33)	Acc@5  94.92 ( 97.43)
Test: [ 30/196]	Time  4.146 ( 4.383)	Loss 5.9598e-01 (4.8761e-01)	Acc@1  86.72 ( 87.37)	Acc@5  97.27 ( 97.73)
Test: [ 40/196]	Time  4.178 ( 4.322)	Loss 5.6757e-01 (5.3622e-01)	Acc@1  86.33 ( 86.21)	Acc@5  96.48 ( 97.48)
Test: [ 50/196]	Time  3.908 ( 4.307)	Loss 2.8338e-01 (5.3801e-01)	Acc@1  93.36 ( 86.07)	Acc@5  98.83 ( 97.48)
Test: [ 60/196]	Time  4.129 ( 4.249)	Loss 6.6021e-01 (5.5656e-01)	Acc@1  80.47 ( 85.65)	Acc@5  96.88 ( 97.47)
Test: [ 70/196]	Time  4.130 ( 4.218)	Loss 6.4056e-01 (5.4179e-01)	Acc@1  81.25 ( 85.92)	Acc@5  98.83 ( 97.58)
Test: [ 80/196]	Time  4.339 ( 4.191)	Loss 9.8676e-01 (5.6029e-01)	Acc@1  74.22 ( 85.49)	Acc@5  94.14 ( 97.41)
Test: [ 90/196]	Time  4.404 ( 4.211)	Loss 1.5782e+00 (5.9838e-01)	Acc@1  63.28 ( 84.58)	Acc@5  89.84 ( 97.03)
Test: [100/196]	Time  4.280 ( 4.215)	Loss 8.5715e-01 (6.3607e-01)	Acc@1  74.61 ( 83.62)	Acc@5  96.09 ( 96.70)
Test: [110/196]	Time  4.198 ( 4.205)	Loss 6.0468e-01 (6.5274e-01)	Acc@1  83.59 ( 83.26)	Acc@5  96.88 ( 96.53)
Test: [120/196]	Time  3.308 ( 4.188)	Loss 1.0206e+00 (6.6438e-01)	Acc@1  76.56 ( 83.17)	Acc@5  92.58 ( 96.37)
Test: [130/196]	Time  4.197 ( 4.181)	Loss 4.2945e-01 (6.8790e-01)	Acc@1  86.33 ( 82.53)	Acc@5  97.66 ( 96.11)
Test: [140/196]	Time  4.216 ( 4.176)	Loss 7.2181e-01 (7.0023e-01)	Acc@1  84.38 ( 82.30)	Acc@5  94.92 ( 95.98)
Test: [150/196]	Time  4.366 ( 4.182)	Loss 7.3896e-01 (7.1621e-01)	Acc@1  83.20 ( 81.96)	Acc@5  95.31 ( 95.77)
Test: [160/196]	Time  3.995 ( 4.189)	Loss 5.3167e-01 (7.2624e-01)	Acc@1  86.72 ( 81.76)	Acc@5  95.70 ( 95.65)
Test: [170/196]	Time  3.970 ( 4.164)	Loss 4.2865e-01 (7.3852e-01)	Acc@1  87.89 ( 81.45)	Acc@5  98.44 ( 95.54)
Test: [180/196]	Time  4.174 ( 4.171)	Loss 1.0352e+00 (7.5269e-01)	Acc@1  72.66 ( 81.10)	Acc@5  95.70 ( 95.43)
Test: [190/196]	Time  4.187 ( 4.168)	Loss 1.1443e+00 (7.5456e-01)	Acc@1  70.70 ( 81.01)	Acc@5  95.70 ( 95.44)
Batch size = 256
Accuracy: 0.81076 Accuracy@5 0.95462
The Elapsed Time is : 6546.894850343
