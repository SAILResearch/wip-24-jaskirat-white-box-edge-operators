nohup: ignoring input
+ main --input_model=roberta-tiny-distilled-mrpc.onnx --output_model=roberta-tiny-distilled-mrpc-int8-sptq.onnx --dataset_location=./glue_data/MRPC --quant_format=QDQ
+ init_params --input_model=roberta-tiny-distilled-mrpc.onnx --output_model=roberta-tiny-distilled-mrpc-int8-sptq.onnx --dataset_location=./glue_data/MRPC --quant_format=QDQ
+ for var in "$@"
+ case $var in
++ echo --input_model=roberta-tiny-distilled-mrpc.onnx
++ cut -f2 -d=
+ input_model=roberta-tiny-distilled-mrpc.onnx
+ for var in "$@"
+ case $var in
++ echo --output_model=roberta-tiny-distilled-mrpc-int8-sptq.onnx
++ cut -f2 -d=
+ output_model=roberta-tiny-distilled-mrpc-int8-sptq.onnx
+ for var in "$@"
+ case $var in
++ echo --dataset_location=./glue_data/MRPC
++ cut -f2 -d=
+ dataset_location=./glue_data/MRPC
+ for var in "$@"
+ case $var in
++ echo --quant_format=QDQ
++ cut -f2 -d=
+ quant_format=QDQ
+ run_tuning
+ [[ roberta-tiny-distilled-mrpc.onnx =~ roberta-tiny ]]
+ model_name_or_path=haisongzhang/roberta-tiny-cased
+ TASK_NAME=mrpc
+ num_heads=8
+ hidden_size=512
+ [[ roberta-tiny-distilled-mrpc.onnx =~ bert-base-uncased ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ roberta-base ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ xlm-roberta-base ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ camembert-base ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ distilbert-base ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ albert-base ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ MiniLM-L6 ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ MiniLM-L12 ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ bert-base-cased ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ xlnet-base-cased ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ bert-mini ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ electra-small-discriminator ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ bart ]]
+ [[ roberta-tiny-distilled-mrpc.onnx =~ deberta ]]
+ python main.py --model_name_or_path haisongzhang/roberta-tiny-cased --model_path roberta-tiny-distilled-mrpc.onnx --output_model roberta-tiny-distilled-mrpc-int8-sptq.onnx --data_path ./glue_data/MRPC --task mrpc --num_heads 8 --hidden_size 512 --quant_format QDQ --tune
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING.format("processor"), FutureWarning)
2023-10-31 15:17:44 [INFO] Start auto tuning.
2023-10-31 15:17:44 [INFO] Execute the tuning process due to detect the evaluation function.
2023-10-31 15:17:44 [INFO] Adaptor has 5 recipes.
2023-10-31 15:17:44 [INFO] 0 recipes specified by user.
2023-10-31 15:17:44 [INFO] 3 recipes require future tuning.
2023-10-31 15:17:44 [INFO] *** Initialize auto tuning
2023-10-31 15:17:44 [INFO] {
2023-10-31 15:17:44 [INFO]     'PostTrainingQuantConfig': {
2023-10-31 15:17:44 [INFO]         'AccuracyCriterion': {
2023-10-31 15:17:44 [INFO]             'criterion': 'relative',
2023-10-31 15:17:44 [INFO]             'higher_is_better': True,
2023-10-31 15:17:44 [INFO]             'tolerable_loss': 0.01,
2023-10-31 15:17:44 [INFO]             'absolute': None,
2023-10-31 15:17:44 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f387d17ef80>>,
2023-10-31 15:17:44 [INFO]             'relative': 0.01
2023-10-31 15:17:44 [INFO]         },
2023-10-31 15:17:44 [INFO]         'approach': 'post_training_static_quant',
2023-10-31 15:17:44 [INFO]         'backend': 'default',
2023-10-31 15:17:44 [INFO]         'calibration_sampling_size': [
2023-10-31 15:17:44 [INFO]             100
2023-10-31 15:17:44 [INFO]         ],
2023-10-31 15:17:44 [INFO]         'device': 'cpu',
2023-10-31 15:17:44 [INFO]         'diagnosis': False,
2023-10-31 15:17:44 [INFO]         'domain': 'auto',
2023-10-31 15:17:44 [INFO]         'example_inputs': None,
2023-10-31 15:17:44 [INFO]         'excluded_precisions': [
2023-10-31 15:17:44 [INFO]         ],
2023-10-31 15:17:44 [INFO]         'framework': 'onnxruntime',
2023-10-31 15:17:44 [INFO]         'inputs': [
2023-10-31 15:17:44 [INFO]         ],
2023-10-31 15:17:44 [INFO]         'model_name': '',
2023-10-31 15:17:44 [INFO]         'ni_workload_name': 'quantization',
2023-10-31 15:17:44 [INFO]         'op_name_dict': None,
2023-10-31 15:17:44 [INFO]         'op_type_dict': None,
2023-10-31 15:17:44 [INFO]         'outputs': [
2023-10-31 15:17:44 [INFO]         ],
2023-10-31 15:17:44 [INFO]         'quant_format': 'QDQ',
2023-10-31 15:17:44 [INFO]         'quant_level': 'auto',
2023-10-31 15:17:44 [INFO]         'recipes': {
2023-10-31 15:17:44 [INFO]             'smooth_quant': False,
2023-10-31 15:17:44 [INFO]             'smooth_quant_args': {
2023-10-31 15:17:44 [INFO]             },
2023-10-31 15:17:44 [INFO]             'layer_wise_quant': False,
2023-10-31 15:17:44 [INFO]             'layer_wise_quant_args': {
2023-10-31 15:17:44 [INFO]             },
2023-10-31 15:17:44 [INFO]             'fast_bias_correction': False,
2023-10-31 15:17:44 [INFO]             'weight_correction': False,
2023-10-31 15:17:44 [INFO]             'gemm_to_matmul': True,
2023-10-31 15:17:44 [INFO]             'graph_optimization_level': None,
2023-10-31 15:17:44 [INFO]             'first_conv_or_matmul_quantization': True,
2023-10-31 15:17:44 [INFO]             'last_conv_or_matmul_quantization': True,
2023-10-31 15:17:44 [INFO]             'pre_post_process_quantization': True,
2023-10-31 15:17:44 [INFO]             'add_qdq_pair_to_weight': False,
2023-10-31 15:17:44 [INFO]             'optypes_to_exclude_output_quant': [
2023-10-31 15:17:44 [INFO]             ],
2023-10-31 15:17:44 [INFO]             'dedicated_qdq_pair': False,
2023-10-31 15:17:44 [INFO]             'rtn_args': {
2023-10-31 15:17:44 [INFO]             },
2023-10-31 15:17:44 [INFO]             'awq_args': {
2023-10-31 15:17:44 [INFO]             },
2023-10-31 15:17:44 [INFO]             'gptq_args': {
2023-10-31 15:17:44 [INFO]             },
2023-10-31 15:17:44 [INFO]             'teq_args': {
2023-10-31 15:17:44 [INFO]             }
2023-10-31 15:17:44 [INFO]         },
2023-10-31 15:17:44 [INFO]         'reduce_range': None,
2023-10-31 15:17:44 [INFO]         'TuningCriterion': {
2023-10-31 15:17:44 [INFO]             'max_trials': 100,
2023-10-31 15:17:44 [INFO]             'objective': [
2023-10-31 15:17:44 [INFO]                 'performance'
2023-10-31 15:17:44 [INFO]             ],
2023-10-31 15:17:44 [INFO]             'strategy': 'basic',
2023-10-31 15:17:44 [INFO]             'strategy_kwargs': None,
2023-10-31 15:17:44 [INFO]             'timeout': 0
2023-10-31 15:17:44 [INFO]         },
2023-10-31 15:17:44 [INFO]         'use_bf16': True
2023-10-31 15:17:44 [INFO]     }
2023-10-31 15:17:44 [INFO] }
2023-10-31 15:17:44 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.
2023-10-31 15:17:44 [WARNING] The model is automatically detected as an NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it
2023-10-31 15:17:44 [WARNING] Graph optimization level is automatically set to ENABLE_EXTENDED. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it
2023-10-31 15:17:46 [INFO] Get FP32 model baseline.
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
2023-10-31 15:17:48 [INFO] Save tuning history to /home/21js160/distillation_roberta/roberta-tiny/nc_workspace/2023-10-31_15-17-36/./history.snapshot.
2023-10-31 15:17:48 [INFO] FP32 baseline is: [Accuracy: 0.8889, Duration (seconds): 2.3047]
2023-10-31 15:17:48 [INFO] Quantize the model with default config.
2023-10-31 15:17:48 [WARNING] Reset `calibration.dataloader.batch_size` field to 5 to make sure the sampling_size is divisible exactly by batch size
2023-10-31 15:18:06 [INFO] |*******Mixed Precision Statistics*******|
2023-10-31 15:18:06 [INFO] +------------------+-------+------+------+
2023-10-31 15:18:06 [INFO] |     Op Type      | Total | INT8 | FP32 |
2023-10-31 15:18:06 [INFO] +------------------+-------+------+------+
2023-10-31 15:18:06 [INFO] |      MatMul      |   13  |  13  |  0   |
2023-10-31 15:18:06 [INFO] |    Attention     |   4   |  4   |  0   |
2023-10-31 15:18:06 [INFO] |      Gather      |   5   |  3   |  2   |
2023-10-31 15:18:06 [INFO] |    Unsqueeze     |   1   |  0   |  1   |
2023-10-31 15:18:06 [INFO] |      Slice       |   1   |  0   |  1   |
2023-10-31 15:18:06 [INFO] |  QuantizeLinear  |   33  |  33  |  0   |
2023-10-31 15:18:06 [INFO] | DequantizeLinear |   57  |  57  |  0   |
2023-10-31 15:18:06 [INFO] +------------------+-------+------+------+
2023-10-31 15:18:06 [INFO] Pass quantize model elapsed time: 17567.41 ms
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/21js160/anaconda3/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
2023-10-31 15:18:09 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.8807|0.8889, Duration (seconds) (int8|fp32): 3.2089|2.3047], Best tune result is: [Accuracy: 0.8807, Duration (seconds): 3.2089]
2023-10-31 15:18:09 [INFO] |**********************Tune Result Statistics**********************|
2023-10-31 15:18:09 [INFO] +--------------------+----------+---------------+------------------+
2023-10-31 15:18:09 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2023-10-31 15:18:09 [INFO] +--------------------+----------+---------------+------------------+
2023-10-31 15:18:09 [INFO] |      Accuracy      | 0.8889   |    0.8807     |     0.8807       |
2023-10-31 15:18:09 [INFO] | Duration (seconds) | 2.3047   |    3.2089     |     3.2089       |
2023-10-31 15:18:09 [INFO] +--------------------+----------+---------------+------------------+
2023-10-31 15:18:09 [INFO] [Strategy] Found a model that meets the accuracy requirements.
2023-10-31 15:18:09 [INFO] Save tuning history to /home/21js160/distillation_roberta/roberta-tiny/nc_workspace/2023-10-31_15-17-36/./history.snapshot.
2023-10-31 15:18:09 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.
2023-10-31 15:18:09 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2023-10-31 15:18:09 [INFO] Save deploy yaml to /home/21js160/distillation_roberta/roberta-tiny/nc_workspace/2023-10-31_15-17-36/deploy.yaml
The Elapsed Time is : 30.01619114726782
